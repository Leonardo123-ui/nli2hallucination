#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†è„šæœ¬ï¼šåˆ†æä¸åŒtask_typeçš„æ•°æ®åˆ†å¸ƒ
è¾“å‡ºæ ¼å¼ï¼šJSON, CSV, XLSX
"""

import pandas as pd
import numpy as np
import json
from collections import defaultdict
import os

def load_data():
    """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
    train_path = "train-00000-of-00001.parquet"
    test_path = "test-00000-of-00001.parquet"
    
    train_df = pd.read_parquet(train_path)
    test_df = pd.read_parquet(test_path)
    
    # æ·»åŠ æ•°æ®é›†æ ‡è¯†
    train_df['dataset'] = 'train'
    test_df['dataset'] = 'test'
    
    combined_df = pd.concat([train_df, test_df], ignore_index=True)
    return combined_df, train_df, test_df

def analyze_task_type_distribution(df):
    """åˆ†æä¸åŒtask_typeçš„è¯¦ç»†åˆ†å¸ƒ"""
    results = {}
    
    # æ€»ä½“ç»Ÿè®¡
    task_types = df['task_type'].unique()
    
    for task_type in task_types:
        task_data = df[df['task_type'] == task_type]
        
        # åŸºæœ¬ç»Ÿè®¡
        basic_stats = {
            'total_samples': len(task_data),
            'percentage': len(task_data) / len(df) * 100,
            'train_samples': len(task_data[task_data['dataset'] == 'train']),
            'test_samples': len(task_data[task_data['dataset'] == 'test'])
        }
        
        # è´¨é‡åˆ†å¸ƒ
        quality_dist = task_data['quality'].value_counts().to_dict()
        quality_percentage = (task_data['quality'].value_counts() / len(task_data) * 100).to_dict()
        
        # æ¨¡å‹åˆ†å¸ƒ
        model_dist = task_data['model'].value_counts().to_dict()
        model_percentage = (task_data['model'].value_counts() / len(task_data) * 100).to_dict()
        
        # å¹»è§‰ç»Ÿè®¡
        hallucination_stats = analyze_hallucinations(task_data)
        
        # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
        text_length_stats = {
            'query_length': {
                'mean': float(task_data['query'].str.len().mean()),
                'median': float(task_data['query'].str.len().median()),
                'min': int(task_data['query'].str.len().min()),
                'max': int(task_data['query'].str.len().max()),
                'std': float(task_data['query'].str.len().std())
            },
            'context_length': {
                'mean': float(task_data['context'].str.len().mean()),
                'median': float(task_data['context'].str.len().median()),
                'min': int(task_data['context'].str.len().min()),
                'max': int(task_data['context'].str.len().max()),
                'std': float(task_data['context'].str.len().std())
            },
            'output_length': {
                'mean': float(task_data['output'].str.len().mean()),
                'median': float(task_data['output'].str.len().median()),
                'min': int(task_data['output'].str.len().min()),
                'max': int(task_data['output'].str.len().max()),
                'std': float(task_data['output'].str.len().std())
            }
        }
        
        # æ¸©åº¦å‚æ•°ç»Ÿè®¡
        temp_stats = {
            'mean': float(task_data['temperature'].mean()),
            'median': float(task_data['temperature'].median()),
            'min': float(task_data['temperature'].min()),
            'max': float(task_data['temperature'].max()),
            'std': float(task_data['temperature'].std())
        }
        
        results[task_type] = {
            'basic_statistics': basic_stats,
            'quality_distribution': {
                'counts': quality_dist,
                'percentages': {k: round(v, 2) for k, v in quality_percentage.items()}
            },
            'model_distribution': {
                'counts': model_dist,
                'percentages': {k: round(v, 2) for k, v in model_percentage.items()}
            },
            'hallucination_statistics': hallucination_stats,
            'text_length_statistics': text_length_stats,
            'temperature_statistics': temp_stats
        }
    
    return results

def analyze_hallucinations(task_data):
    """åˆ†æå¹»è§‰æ ‡ç­¾ç»Ÿè®¡"""
    evident_conflict_counts = []
    baseless_info_counts = []
    
    for _, row in task_data.iterrows():
        labels = row['hallucination_labels_processed']
        if isinstance(labels, dict):
            evident_conflict_counts.append(labels.get('evident_conflict', 0))
            baseless_info_counts.append(labels.get('baseless_info', 0))
        else:
            evident_conflict_counts.append(0)
            baseless_info_counts.append(0)
    
    evident_conflict_counts = np.array(evident_conflict_counts)
    baseless_info_counts = np.array(baseless_info_counts)
    
    return {
        'evident_conflict': {
            'samples_with_conflict': int(np.sum(evident_conflict_counts > 0)),
            'percentage_with_conflict': float(np.sum(evident_conflict_counts > 0) / len(task_data) * 100),
            'total_conflicts': int(evident_conflict_counts.sum()),
            'mean_per_sample': float(evident_conflict_counts.mean())
        },
        'baseless_info': {
            'samples_with_baseless': int(np.sum(baseless_info_counts > 0)),
            'percentage_with_baseless': float(np.sum(baseless_info_counts > 0) / len(task_data) * 100),
            'total_baseless': int(baseless_info_counts.sum()),
            'mean_per_sample': float(baseless_info_counts.mean())
        },
        'any_hallucination': {
            'samples_with_any': int(np.sum((evident_conflict_counts > 0) | (baseless_info_counts > 0))),
            'percentage_with_any': float(np.sum((evident_conflict_counts > 0) | (baseless_info_counts > 0)) / len(task_data) * 100)
        }
    }

def create_summary_table(results):
    """åˆ›å»ºæ±‡æ€»è¡¨æ ¼ç”¨äºCSVå’ŒXLSXå¯¼å‡º"""
    summary_data = []
    
    for task_type, stats in results.items():
        row = {
            'Task_Type': task_type,
            'Total_Samples': stats['basic_statistics']['total_samples'],
            'Percentage': round(stats['basic_statistics']['percentage'], 2),
            'Train_Samples': stats['basic_statistics']['train_samples'],
            'Test_Samples': stats['basic_statistics']['test_samples'],
            
            # è´¨é‡åˆ†å¸ƒ
            'Good_Quality': stats['quality_distribution']['counts'].get('good', 0),
            'Good_Quality_Pct': stats['quality_distribution']['percentages'].get('good', 0),
            
            # å¹»è§‰ç»Ÿè®¡
            'Evident_Conflict_Samples': stats['hallucination_statistics']['evident_conflict']['samples_with_conflict'],
            'Evident_Conflict_Pct': round(stats['hallucination_statistics']['evident_conflict']['percentage_with_conflict'], 2),
            'Baseless_Info_Samples': stats['hallucination_statistics']['baseless_info']['samples_with_baseless'],
            'Baseless_Info_Pct': round(stats['hallucination_statistics']['baseless_info']['percentage_with_baseless'], 2),
            'Any_Hallucination_Samples': stats['hallucination_statistics']['any_hallucination']['samples_with_any'],
            'Any_Hallucination_Pct': round(stats['hallucination_statistics']['any_hallucination']['percentage_with_any'], 2),
            
            # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
            'Avg_Query_Length': round(stats['text_length_statistics']['query_length']['mean'], 1),
            'Avg_Context_Length': round(stats['text_length_statistics']['context_length']['mean'], 1),
            'Avg_Output_Length': round(stats['text_length_statistics']['output_length']['mean'], 1),
            
            # æ¸©åº¦ç»Ÿè®¡
            'Avg_Temperature': round(stats['temperature_statistics']['mean'], 3),
        }
        summary_data.append(row)
    
    return pd.DataFrame(summary_data)

def export_results(results, summary_df):
    """å¯¼å‡ºç»“æœåˆ°å¤šç§æ ¼å¼"""
    # 1. å¯¼å‡ºè¯¦ç»†JSON
    with open('task_type_analysis_detailed.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # 2. å¯¼å‡ºæ±‡æ€»CSV
    summary_df.to_csv('task_type_analysis_summary.csv', index=False, encoding='utf-8')
    
    # 3. å¯¼å‡ºæ±‡æ€»XLSX (éœ€è¦å®‰è£…openpyxl)
    try:
        with pd.ExcelWriter('task_type_analysis_summary.xlsx', engine='openpyxl') as writer:
            summary_df.to_excel(writer, sheet_name='Task_Type_Summary', index=False)
            
            # ä¸ºæ¯ä¸ªtask_typeåˆ›å»ºè¯¦ç»†sheet
            for task_type, stats in results.items():
                # åˆ›å»ºè¯¦ç»†ç»Ÿè®¡è¡¨
                detail_data = []
                
                # åŸºæœ¬ç»Ÿè®¡
                detail_data.append(['åŸºæœ¬ç»Ÿè®¡', '', ''])
                detail_data.append(['æ€»æ ·æœ¬æ•°', stats['basic_statistics']['total_samples'], ''])
                detail_data.append(['ç™¾åˆ†æ¯”', f"{stats['basic_statistics']['percentage']:.2f}%", ''])
                detail_data.append(['è®­ç»ƒæ ·æœ¬', stats['basic_statistics']['train_samples'], ''])
                detail_data.append(['æµ‹è¯•æ ·æœ¬', stats['basic_statistics']['test_samples'], ''])
                detail_data.append(['', '', ''])
                
                # è´¨é‡åˆ†å¸ƒ
                detail_data.append(['è´¨é‡åˆ†å¸ƒ', 'æ•°é‡', 'ç™¾åˆ†æ¯”'])
                for quality, count in stats['quality_distribution']['counts'].items():
                    pct = stats['quality_distribution']['percentages'][quality]
                    detail_data.append([quality, count, f"{pct:.2f}%"])
                detail_data.append(['', '', ''])
                
                # æ¨¡å‹åˆ†å¸ƒ
                detail_data.append(['æ¨¡å‹åˆ†å¸ƒ', 'æ•°é‡', 'ç™¾åˆ†æ¯”'])
                for model, count in stats['model_distribution']['counts'].items():
                    pct = stats['model_distribution']['percentages'][model]
                    detail_data.append([model, count, f"{pct:.2f}%"])
                
                detail_df = pd.DataFrame(detail_data, columns=['æŒ‡æ ‡', 'å€¼', 'ç™¾åˆ†æ¯”'])
                detail_df.to_excel(writer, sheet_name=f'{task_type}_è¯¦ç»†', index=False)
        
        print("âœ… æˆåŠŸå¯¼å‡º XLSX æ–‡ä»¶")
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… openpyxl æ‰èƒ½å¯¼å‡º XLSX æ–‡ä»¶: pip install openpyxl")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®...")
    df, train_df, test_df = load_data()
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: æ€»æ ·æœ¬ {len(df)}, è®­ç»ƒ {len(train_df)}, æµ‹è¯• {len(test_df)}")
    
    print("ğŸ”„ æ­£åœ¨åˆ†æä¸åŒtask_typeçš„åˆ†å¸ƒ...")
    results = analyze_task_type_distribution(df)
    
    print("ğŸ”„ æ­£åœ¨åˆ›å»ºæ±‡æ€»è¡¨æ ¼...")
    summary_df = create_summary_table(results)
    
    print("ğŸ”„ æ­£åœ¨å¯¼å‡ºç»“æœ...")
    export_results(results, summary_df)
    
    print("\nâœ… åˆ†æå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - task_type_analysis_detailed.json (è¯¦ç»†JSONæ•°æ®)")
    print("   - task_type_analysis_summary.csv (æ±‡æ€»CSVè¡¨æ ¼)")
    print("   - task_type_analysis_summary.xlsx (æ±‡æ€»Excelè¡¨æ ¼ï¼ŒåŒ…å«è¯¦ç»†sheets)")
    
    print("\nğŸ“ˆ å¿«é€Ÿæ±‡æ€»:")
    print(summary_df[['Task_Type', 'Total_Samples', 'Percentage', 'Any_Hallucination_Pct']].to_string(index=False))

if __name__ == "__main__":
    main()