import os
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, confusion_matrix, 
    roc_auc_score, roc_curve, precision_recall_curve, classification_report
)
import json
import argparse
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HallucinationDataset(Dataset):
    """å¹»è§‰æ£€æµ‹æ•°æ®é›†ç±»"""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def load_test_data(data_path):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    logger.info(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {data_path}")
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†
    full_df = pd.read_excel(data_path, sheet_name='NLIæ•°æ®é›†')
    train_df = full_df[full_df['split'] == 'train']
    test_df = full_df[full_df['split'] == 'test']
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_df)}, æµ‹è¯•é›†å¤§å°: {len(test_df)}")
    
    return train_df, test_df

def prepare_texts(df, input_format='context_output'):
    """å‡†å¤‡è¾“å…¥æ–‡æœ¬"""
    if input_format == 'context_output':
        texts = [f"{row['context']} [SEP] {row['output']}" for _, row in df.iterrows()]
    elif input_format == 'context_only':
        texts = df['context'].tolist()
    elif input_format == 'output_only':
        texts = df['output'].tolist()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼: {input_format}")
    
    return texts

def predict_batch(model, dataloader, device):
    """æ‰¹é‡é¢„æµ‹"""
    model.eval()
    all_predictions = []
    all_probabilities = []
    all_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            # è·å–æ¦‚ç‡
            probabilities = torch.softmax(logits, dim=-1)
            predictions = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    return np.array(all_predictions), np.array(all_probabilities), np.array(all_labels)

def calculate_detailed_metrics(y_true, y_pred, y_prob):
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
    # åŸºæœ¬æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)
    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # å„ç±»åˆ«æŒ‡æ ‡
    no_hall_precision, hall_precision = precision
    no_hall_recall, hall_recall = recall
    no_hall_f1, hall_f1 = f1
    no_hall_support, hall_support = support
    
    # ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    
    # å‡é˜³æ€§ç‡å’Œå‡é˜´æ€§ç‡
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
    
    # AUCåˆ†æ•°
    try:
        auc_score = roc_auc_score(y_true, y_prob[:, 1])
    except:
        auc_score = 0.0
    
    # å¹³è¡¡å‡†ç¡®ç‡
    balanced_accuracy = (sensitivity + specificity) / 2
    
    return {
        # æ€»ä½“æŒ‡æ ‡
        'accuracy': accuracy,
        'balanced_accuracy': balanced_accuracy,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'auc_score': auc_score,
        
        # ç±»åˆ«0ï¼ˆæ— å¹»è§‰ï¼‰æŒ‡æ ‡
        'no_hallucination': {
            'precision': no_hall_precision,
            'recall': no_hall_recall,
            'f1_score': no_hall_f1,
            'support': int(no_hall_support)
        },
        
        # ç±»åˆ«1ï¼ˆæœ‰å¹»è§‰ï¼‰æŒ‡æ ‡  
        'hallucination': {
            'precision': hall_precision,
            'recall': hall_recall,
            'f1_score': hall_f1,
            'support': int(hall_support)
        },
        
        # æ··æ·†çŸ©é˜µç›¸å…³
        'confusion_matrix': {
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp)
        },
        
        # ç‡æŒ‡æ ‡
        'specificity': specificity,
        'sensitivity': sensitivity,
        'false_positive_rate': fpr,
        'false_negative_rate': fnr,
        'miss_rate': fnr,  # æ¼æ£€ç‡
        'fall_out': fpr    # è¯¯æŠ¥ç‡
    }

def plot_confusion_matrix(y_true, y_pred, output_dir):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_true, y_pred)
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['No Hallucination', 'Hallucination'],
                yticklabels=['No Hallucination', 'Hallucination'])
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()

def plot_roc_curve(y_true, y_prob, output_dir):
    """ç»˜åˆ¶ROCæ›²çº¿"""
    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])
    auc_score = roc_auc_score(y_true, y_prob[:, 1])
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.4f})')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/roc_curve.png', dpi=300, bbox_inches='tight')
    plt.close()

def plot_precision_recall_curve(y_true, y_prob, output_dir):
    """ç»˜åˆ¶ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿"""
    precision, recall, _ = precision_recall_curve(y_true, y_prob[:, 1])
    
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, linewidth=2, label='PR Curve')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
    plt.close()

def analyze_errors(df, predictions, labels, output_dir):
    """åˆ†æé”™è¯¯æ ·æœ¬"""
    df_analysis = df.copy()
    df_analysis['predicted'] = predictions
    df_analysis['correct'] = (predictions == labels)
    
    # é”™è¯¯æ ·æœ¬
    errors = df_analysis[~df_analysis['correct']]
    
    # å‡é˜³æ€§ï¼ˆè¯¯æŠ¥ï¼‰ï¼šå®é™…æ— å¹»è§‰ï¼Œé¢„æµ‹æœ‰å¹»è§‰
    false_positives = errors[errors['label'] == 0]
    
    # å‡é˜´æ€§ï¼ˆæ¼æŠ¥ï¼‰ï¼šå®é™…æœ‰å¹»è§‰ï¼Œé¢„æµ‹æ— å¹»è§‰  
    false_negatives = errors[errors['label'] == 1]
    
    # ä¿å­˜é”™è¯¯åˆ†æ
    error_analysis = {
        'total_errors': len(errors),
        'false_positives': len(false_positives),
        'false_negatives': len(false_negatives),
        'error_rate': len(errors) / len(df_analysis)
    }
    
    # ä¿å­˜é”™è¯¯æ ·æœ¬ï¼ˆå‰10ä¸ªï¼‰
    if len(false_positives) > 0:
        fp_samples = false_positives.head(10)[['id', 'context', 'output', 'label', 'predicted']].to_dict('records')
        error_analysis['false_positive_samples'] = fp_samples
    
    if len(false_negatives) > 0:
        fn_samples = false_negatives.head(10)[['id', 'context', 'output', 'label', 'predicted']].to_dict('records')
        error_analysis['false_negative_samples'] = fn_samples
    
    with open(f'{output_dir}/error_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(error_analysis, f, ensure_ascii=False, indent=2)
    
    return error_analysis

def test_model(
    model_path,
    data_path='../summary_nli_hallucination_dataset.xlsx',
    output_dir='./test_results',
    input_format='context_output',
    max_length=512,
    batch_size=32
):
    """æµ‹è¯•æ¨¡å‹"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    
    # åŠ è½½æ•°æ®
    train_df, test_df = load_test_data(data_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_texts = prepare_texts(test_df, input_format)
    test_labels = test_df['label'].tolist()
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    test_dataset = HallucinationDataset(test_texts, test_labels, tokenizer, max_length)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # è¿›è¡Œé¢„æµ‹
    logger.info("æ­£åœ¨è¿›è¡Œé¢„æµ‹...")
    predictions, probabilities, true_labels = predict_batch(model, test_dataloader, device)
    
    # è®¡ç®—æŒ‡æ ‡
    logger.info("æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    metrics = calculate_detailed_metrics(true_labels, predictions, probabilities)
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    class_report = classification_report(
        true_labels, predictions, 
        target_names=['No Hallucination', 'Hallucination'],
        output_dict=True
    )
    
    # ç»˜åˆ¶å›¾è¡¨
    logger.info("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_confusion_matrix(true_labels, predictions, output_dir)
    plot_roc_curve(true_labels, probabilities, output_dir)
    plot_precision_recall_curve(true_labels, probabilities, output_dir)
    
    # é”™è¯¯åˆ†æ
    logger.info("æ­£åœ¨è¿›è¡Œé”™è¯¯åˆ†æ...")
    error_analysis = analyze_errors(test_df, predictions, true_labels, output_dir)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    results = {
        'model_path': model_path,
        'test_time': datetime.now().isoformat(),
        'test_size': len(test_dataset),
        'input_format': input_format,
        'detailed_metrics': metrics,
        'classification_report': class_report,
        'error_analysis_summary': error_analysis
    }
    
    with open(f'{output_dir}/test_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    results_df = test_df.copy()
    results_df['predicted_label'] = predictions
    results_df['hallucination_probability'] = probabilities[:, 1]
    results_df['correct_prediction'] = (predictions == true_labels)
    
    results_df.to_excel(f'{output_dir}/detailed_predictions.xlsx', index=False)
    
    logger.info("æµ‹è¯•å®Œæˆï¼")
    
    return metrics, results

def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•å¹»è§‰æ£€æµ‹æ¨¡å‹')
    parser.add_argument('--model_path', required=True,
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_path', default='../summary_nli_hallucination_dataset.xlsx',
                       help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', default='./test_results',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--input_format', default='context_output',
                       choices=['context_output', 'context_only', 'output_only'],
                       help='è¾“å…¥æ–‡æœ¬æ ¼å¼')
    parser.add_argument('--max_length', type=int, default=512,
                       help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæµ‹è¯•
    metrics, results = test_model(
        model_path=args.model_path,
        data_path=args.data_path,
        output_dir=args.output_dir,
        input_format=args.input_format,
        max_length=args.max_length,
        batch_size=args.batch_size
    )
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("å¹»è§‰æ£€æµ‹æ¨¡å‹æµ‹è¯•ç»“æœ")
    print("="*60)
    
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½æŒ‡æ ‡:")
    print(f"å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.4f}")
    print(f"å¹³è¡¡å‡†ç¡®ç‡ (Balanced Accuracy): {metrics['balanced_accuracy']:.4f}")
    print(f"å®å¹³å‡ç²¾ç¡®ç‡: {metrics['macro_precision']:.4f}")
    print(f"å®å¹³å‡å¬å›ç‡: {metrics['macro_recall']:.4f}")
    print(f"å®å¹³å‡F1åˆ†æ•°: {metrics['macro_f1']:.4f}")
    print(f"AUCåˆ†æ•°: {metrics['auc_score']:.4f}")
    
    print(f"\nğŸ” æ— å¹»è§‰ç±»åˆ« (æ ‡ç­¾0):")
    no_hall = metrics['no_hallucination']
    print(f"ç²¾ç¡®ç‡: {no_hall['precision']:.4f}")
    print(f"å¬å›ç‡: {no_hall['recall']:.4f}")
    print(f"F1åˆ†æ•°: {no_hall['f1_score']:.4f}")
    print(f"æ ·æœ¬æ•°: {no_hall['support']}")
    
    print(f"\nâš ï¸  æœ‰å¹»è§‰ç±»åˆ« (æ ‡ç­¾1):")
    hall = metrics['hallucination']
    print(f"ç²¾ç¡®ç‡: {hall['precision']:.4f}")
    print(f"å¬å›ç‡: {hall['recall']:.4f}")
    print(f"F1åˆ†æ•°: {hall['f1_score']:.4f}")
    print(f"æ ·æœ¬æ•°: {hall['support']}")
    
    print(f"\nğŸ“ˆ å…³é”®å¹»è§‰æ£€æµ‹æŒ‡æ ‡:")
    print(f"æ•æ„Ÿæ€§/å¬å›ç‡ (Sensitivity): {metrics['sensitivity']:.4f}")
    print(f"ç‰¹å¼‚æ€§ (Specificity): {metrics['specificity']:.4f}")
    print(f"å‡é˜³æ€§ç‡/è¯¯æŠ¥ç‡ (FPR): {metrics['false_positive_rate']:.4f}")
    print(f"å‡é˜´æ€§ç‡/æ¼æ£€ç‡ (FNR): {metrics['false_negative_rate']:.4f}")
    
    cm = metrics['confusion_matrix']
    print(f"\nğŸ”¢ æ··æ·†çŸ©é˜µ:")
    print(f"çœŸé˜´æ€§ (TN): {cm['true_negatives']}")
    print(f"å‡é˜³æ€§ (FP): {cm['false_positives']}")
    print(f"å‡é˜´æ€§ (FN): {cm['false_negatives']}")
    print(f"çœŸé˜³æ€§ (TP): {cm['true_positives']}")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
    print("="*60)

if __name__ == "__main__":
    main()