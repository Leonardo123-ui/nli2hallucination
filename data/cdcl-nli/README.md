# CDCL-NLI å¹»è§‰æ£€æµ‹é€‚é…

å°†å¹»è§‰æ£€æµ‹æ•°æ®é€‚é…åˆ° CDCL-NLI æ¨¡å‹ï¼Œä½¿ç”¨ RST (ä¿®è¾ç»“æ„æ ‘) å’Œå›¾ç¥ç»ç½‘ç»œè¿›è¡Œå¹»è§‰æ£€æµ‹ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å°†å¹»è§‰æ£€æµ‹ä»»åŠ¡é€‚é…åˆ° CDCL-NLI (Cross-Document Cross-Lingual NLI) æ¡†æ¶ï¼š

1. **è¾“å…¥æ•°æ®**: å¹»è§‰æ£€æµ‹æ•°æ®é›† (context + output + label)
2. **å¤„ç†æµç¨‹**:
   - å°† context ä½œä¸º premise (å‰æ)
   - å°† output ä½œä¸º hypothesis (å‡è®¾)
   - ä½¿ç”¨ DM-RST æ¨¡å‹æå–ä¿®è¾ç»“æ„æ ‘
   - ä½¿ç”¨ ModernBERT ç”ŸæˆèŠ‚ç‚¹ embeddings
   - è®¡ç®—è¯æ±‡é“¾ï¼ˆlexical chainsï¼‰çŸ©é˜µ
3. **è¾“å‡º**: é€‚ç”¨äº CDCL-NLI å›¾ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ•°æ®

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
cdcl-nli/
â”œâ”€â”€ convert_hallucination_data.py    # æ•°æ®æ ¼å¼è½¬æ¢è„šæœ¬
â”œâ”€â”€ arrange_hallucination_data.py    # ä¸»æ•°æ®å¤„ç†è„šæœ¬
â”œâ”€â”€ run_pipeline.sh                   # ä¸€é”®è¿è¡Œè„šæœ¬
â”œâ”€â”€ README.md                         # æœ¬æ–‡ä»¶
â”‚
â”œâ”€â”€ data/                             # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ hallucination_train.json     # è½¬æ¢åçš„è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ hallucination_test.json      # è½¬æ¢åçš„æµ‹è¯•æ•°æ®
â”‚   â”‚
â”‚   â”œâ”€â”€ train/                        # è®­ç»ƒé›†å¤„ç†ç»“æœ
â”‚   â”‚   â”œâ”€â”€ rst_result.jsonl         # RST åˆ†æç»“æœ
â”‚   â”‚   â”œâ”€â”€ new_rst_result.jsonl     # é‡å†™çš„ RST ç»“æœ
â”‚   â”‚   â””â”€â”€ node_embeddings.npz      # èŠ‚ç‚¹ embeddings
â”‚   â”‚
â”‚   â”œâ”€â”€ test/                         # æµ‹è¯•é›†å¤„ç†ç»“æœ
â”‚   â”‚   â”œâ”€â”€ rst_result.jsonl
â”‚   â”‚   â”œâ”€â”€ new_rst_result.jsonl
â”‚   â”‚   â””â”€â”€ node_embeddings.npz
â”‚   â”‚
â”‚   â””â”€â”€ graph_info/                   # å›¾ç»“æ„ä¿¡æ¯
â”‚       â”œâ”€â”€ train/
â”‚       â”‚   â””â”€â”€ lexical_matrixes.pkl # è¯æ±‡é“¾çŸ©é˜µ (è®­ç»ƒé›†)
â”‚       â””â”€â”€ test/
â”‚           â””â”€â”€ lexical_matrixes.pkl # è¯æ±‡é“¾çŸ©é˜µ (æµ‹è¯•é›†)
â”‚
â””â”€â”€ DM_RST/                           # RST æ¨¡å‹æ¨¡å— (è½¯é“¾æ¥)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

1. **Python ç¯å¢ƒ**:
   ```bash
   python >= 3.8
   CUDA å¯ç”¨ï¼ˆæ¨èï¼‰
   ```

2. **ä¾èµ–åŒ…**:
   ```bash
   pip install torch transformers pandas numpy nltk tqdm
   ```

3. **DM-RST æ¨¡å‹**:
   - æ¨¡å‹å·²åœ¨ `/mnt/nlp/yuanmengying/CDCL-NLI/data/DM_RST.py`
   - è„šæœ¬ä¼šè‡ªåŠ¨å¼•ç”¨

4. **ModernBERT æ¨¡å‹**:
   - è·¯å¾„: `/mnt/nlp/yuanmengying/nli2hallucination/models/modern-bert_large`

### è¿è¡Œæ­¥éª¤

#### æ­¥éª¤ 1: è½¬æ¢æ•°æ®æ ¼å¼

```bash
cd /mnt/nlp/yuanmengying/nli2hallucination/data/cdcl-nli

# è½¬æ¢å¹»è§‰æ£€æµ‹æ•°æ®ä¸º NLI æ ¼å¼
python convert_hallucination_data.py \
  --excel_path ../../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./data \
  --create_sample
```

**è¾“å‡º**:
- `data/hallucination_train.json` (4,758 æ ·æœ¬)
- `data/hallucination_test.json` (900 æ ·æœ¬)
- `data/hallucination_train_sample.json` (100 æ ·æœ¬ï¼Œç”¨äºæµ‹è¯•)
- `data/hallucination_test_sample.json` (50 æ ·æœ¬ï¼Œç”¨äºæµ‹è¯•)

#### æ­¥éª¤ 2: å¤„ç†æ•°æ®ï¼ˆRST + Embeddings + è¯æ±‡é“¾ï¼‰

```bash
# å®Œæ•´å¤„ç†ï¼ˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
python arrange_hallucination_data.py
```

**å¤„ç†æ—¶é—´ä¼°ç®—**:
- è®­ç»ƒé›† (4,758 æ ·æœ¬): ~3-4 å°æ—¶
- æµ‹è¯•é›† (900 æ ·æœ¬): ~30-40 åˆ†é’Ÿ

**å¤„ç†æµç¨‹**:
1. åŠ è½½è½¬æ¢åçš„ JSON æ•°æ®
2. ä½¿ç”¨ DM-RST æ¨¡å‹æå–ä¿®è¾ç»“æ„æ ‘
3. ä½¿ç”¨ ModernBERT ç”ŸæˆèŠ‚ç‚¹ embeddings
4. è®¡ç®—è¯æ±‡é“¾çŸ©é˜µï¼ˆåŸºäºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰

#### æ­¥éª¤ 3: æŸ¥çœ‹ç»“æœ

```python
import json
import torch
import pickle

# æŸ¥çœ‹ RST ç»“æœ
with open('./data/train/rst_result.jsonl', 'r') as f:
    line = f.readline()
    rst_result = json.loads(line)
    print("RST ç»“æœç¤ºä¾‹:")
    print(json.dumps(rst_result, indent=2))

# æŸ¥çœ‹ embeddings
embeddings = torch.load('./data/train/node_embeddings.npz')
print(f"\nEmbeddings æ•°é‡: {len(embeddings)}")
print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ premise èŠ‚ç‚¹æ•°: {len(embeddings[0]['premise'])}")

# æŸ¥çœ‹è¯æ±‡é“¾çŸ©é˜µ
with open('./data/graph_info/train/lexical_matrixes.pkl', 'rb') as f:
    matrices = pickle.load(f)
    print(f"\nè¯æ±‡é“¾çŸ©é˜µæ•°é‡: {len(matrices)}")
    print(f"ç¬¬ä¸€ä¸ªçŸ©é˜µå½¢çŠ¶: {matrices[0].shape}")
```

## ğŸ“ æ•°æ®æ ¼å¼è¯´æ˜

### è¾“å…¥æ•°æ®ï¼ˆExcelï¼‰

| åˆ—å | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| id | æ ·æœ¬ID | `summary_train_0` |
| context | ä¸Šä¸‹æ–‡ï¼ˆé•¿æ–‡æœ¬ï¼‰ | `Seventy years ago...` |
| output | ç”Ÿæˆçš„æ‘˜è¦ | `The Anne Frank House...` |
| label | æ ‡ç­¾ï¼ˆ0=æ— å¹»è§‰, 1=æœ‰å¹»è§‰ï¼‰ | `0` |
| split | æ•°æ®é›†åˆ’åˆ† | `train` / `test` |
| task_type | ä»»åŠ¡ç±»å‹ | `Summary` |

### è½¬æ¢åæ•°æ®ï¼ˆJSONï¼‰

```json
{
  "news1_origin": "context text...",  // åŸå§‹ context
  "news2_origin": "output text...",   // åŸå§‹ output
  "label": 0,                         // NLI æ ‡ç­¾ (0=entailment, 2=contradiction)
  "original_label": 0,                // åŸå§‹å¹»è§‰æ ‡ç­¾ (0=æ— å¹»è§‰, 1=æœ‰å¹»è§‰)
  "id": "summary_train_0",
  "task_type": "Summary"
}
```

**æ ‡ç­¾æ˜ å°„**:
- æ— å¹»è§‰ (0) â†’ entailment (0) - output ä¸ context ä¸€è‡´
- æœ‰å¹»è§‰ (1) â†’ contradiction (2) - output ä¸ context çŸ›ç›¾

### RST ç»“æœï¼ˆJSONLï¼‰

æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ï¼š

```json
{
  "pre_node_number": [...],      // premise èŠ‚ç‚¹ç¼–å·
  "pre_node_string": [...],      // premise èŠ‚ç‚¹å­—ç¬¦ä¸²
  "pre_node_relations": [...],   // premise èŠ‚ç‚¹å…³ç³»
  "pre_tree": [...],             // premise æ ‘ç»“æ„
  "pre_leaf_node": [...],        // premise å¶å­èŠ‚ç‚¹
  "pre_parent_dict": {...},      // premise çˆ¶èŠ‚ç‚¹å­—å…¸
  "hyp_node_number": [...],      // hypothesis èŠ‚ç‚¹ç¼–å·ï¼ˆç±»ä¼¼ï¼‰
  "hyp_node_string": [...],
  "hyp_node_relations": [...],
  "hyp_tree": [...],
  "hyp_leaf_node": [...],
  "hyp_parent_dict": {...}
}
```

### èŠ‚ç‚¹ Embeddingsï¼ˆ.npzï¼‰

```python
[
  {
    "premise": [
      (node_id, embedding_array, text_string),
      ...
    ],
    "hypothesis": [
      (node_id, embedding_array, text_string),
      ...
    ]
  },
  ...
]
```

### è¯æ±‡é“¾çŸ©é˜µï¼ˆ.pklï¼‰

```python
[
  np.array([[0.0, 0.1, ...],  # premise èŠ‚ç‚¹ 0 ä¸ hypothesis å„èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦
            [0.2, 0.0, ...],  # premise èŠ‚ç‚¹ 1 ä¸ hypothesis å„èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦
            ...]),
  ...
]
```

## ğŸ”§ è„šæœ¬å‚æ•°è¯´æ˜

### `convert_hallucination_data.py`

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--excel_path` | Excel æ•°æ®æ–‡ä»¶è·¯å¾„ | `../../summary_nli_hallucination_dataset.xlsx` |
| `--output_dir` | è¾“å‡ºç›®å½• | `./data` |
| `--create_sample` | æ˜¯å¦åˆ›å»ºå°æ ·æœ¬æ•°æ® | `False` |
| `--sample_size` | å°æ ·æœ¬å¤§å° | `100` |

### `arrange_hallucination_data.py`

ä¸»è¦é…ç½®åœ¨è„šæœ¬å†…éƒ¨ï¼ˆ`if __name__ == "__main__"` éƒ¨åˆ†ï¼‰ï¼š

```python
MODEL_PATH = "/mnt/nlp/yuanmengying/nli2hallucination/models/modern-bert_large"
OVERALL_SAVE_DIR = "/mnt/nlp/yuanmengying/nli2hallucination/data/cdcl-nli/data"
GRAPH_INFOS_DIR = "/mnt/nlp/yuanmengying/nli2hallucination/data/cdcl-nli/data/graph_info"
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### 1. å¿«é€Ÿæµ‹è¯•

ä½¿ç”¨å°æ ·æœ¬æ•°æ®å¿«é€ŸéªŒè¯æµç¨‹ï¼š

```bash
# åˆ›å»ºå°æ ·æœ¬
python convert_hallucination_data.py --create_sample --sample_size 10

# ä¿®æ”¹ arrange_hallucination_data.py ä¸­çš„æ•°æ®è·¯å¾„ä¸ºæ ·æœ¬è·¯å¾„
# TRAIN_DATA_PATH = ".../hallucination_train_sample.json"
# TEST_DATA_PATH = ".../hallucination_test_sample.json"

# è¿è¡Œå¤„ç†
python arrange_hallucination_data.py
```

### 2. åˆ†æ­¥å¤„ç†

å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥åˆ†æ­¥å¤„ç†ï¼š

```python
# 1. åªè¿è¡Œ RST åˆ†æ
data_processor = HallucinationDataProcessor(True, OVERALL_SAVE_DIR, "train")
train_data, train_rst_result = load_all_data(
    data_processor, TRAIN_DATA_PATH, TRAIN_RST_RESULT_PATH
)

# 2. è¿è¡Œ embedding ç”Ÿæˆ
embedder = ModernBERTEmbedder(MODEL_PATH, GRAPH_INFOS_DIR, "train", True)
# ... (åç»­æ­¥éª¤)
```

### 3. è°ƒæ•´æ‰¹æ¬¡å¤§å°

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå‡å°æ‰¹æ¬¡å¤§å°ï¼š

```python
# åœ¨ get_modernbert_embeddings_in_batches ä¸­
batch_size = 64  # ä» 128 å‡å°‘åˆ° 64
```

### 4. è°ƒæ•´è¯æ±‡é“¾é˜ˆå€¼

```python
# åœ¨ find_lexical_chains ä¸­
threshold = 0.7  # ä» 0.8 é™ä½åˆ° 0.7ï¼Œä¼šæœ‰æ›´å¤šè¯æ±‡é“¾è¿æ¥
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: CUDA å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å°æ‰¹æ¬¡å¤§å°
batch_size = 32  # æˆ–æ›´å°

# æ¸…ç† GPU ç¼“å­˜
import torch
torch.cuda.empty_cache()
```

### Q2: RST æ¨¡å‹æ— æ³•åŠ è½½

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ DM-RST è·¯å¾„
ls /mnt/nlp/yuanmengying/CDCL-NLI/data/DM_RST.py

# ç¡®ä¿ Python è·¯å¾„æ­£ç¡®
export PYTHONPATH=$PYTHONPATH:/mnt/nlp/yuanmengying/CDCL-NLI
```

### Q3: ModernBERT æ¨¡å‹åŠ è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls /mnt/nlp/yuanmengying/nli2hallucination/models/modern-bert_large

# æµ‹è¯•æ¨¡å‹åŠ è½½
python -c "from transformers import AutoTokenizer, AutoModel; \
tokenizer = AutoTokenizer.from_pretrained('/mnt/nlp/yuanmengying/nli2hallucination/models/modern-bert_large'); \
model = AutoModel.from_pretrained('/mnt/nlp/yuanmengying/nli2hallucination/models/modern-bert_large'); \
print('æ¨¡å‹åŠ è½½æˆåŠŸ')"
```

### Q4: å¤„ç†é€Ÿåº¦å¤ªæ…¢

**ä¼˜åŒ–å»ºè®®**:
1. ä½¿ç”¨ GPUï¼ˆç¡®ä¿ `torch.cuda.is_available()` è¿”å› Trueï¼‰
2. å¢å¤§æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
3. ä½¿ç”¨å°æ ·æœ¬å…ˆæµ‹è¯•
4. è€ƒè™‘å¹¶è¡Œå¤„ç†ï¼ˆåˆ†å‰²æ•°æ®é›†ï¼‰

## ğŸ“Š æ€§èƒ½å‚è€ƒ

| æ•°æ®é›† | æ ·æœ¬æ•° | RST æ—¶é—´ | Embedding æ—¶é—´ | è¯æ±‡é“¾æ—¶é—´ | æ€»æ—¶é—´ |
|--------|--------|---------|--------------|----------|--------|
| è®­ç»ƒé›† | 4,758 | ~2å°æ—¶ | ~1å°æ—¶ | ~30åˆ†é’Ÿ | ~3.5å°æ—¶ |
| æµ‹è¯•é›† | 900 | ~20åˆ†é’Ÿ | ~10åˆ†é’Ÿ | ~5åˆ†é’Ÿ | ~35åˆ†é’Ÿ |
| å°æ ·æœ¬(100) | 100 | ~3åˆ†é’Ÿ | ~1åˆ†é’Ÿ | ~0.5åˆ†é’Ÿ | ~4.5åˆ†é’Ÿ |

*åŸºäº NVIDIA A100 40GB GPU çš„ä¼°ç®—æ—¶é—´

## ğŸ”— ç›¸å…³èµ„æº

- [CDCL-NLI åŸå§‹é¡¹ç›®](https://github.com/...)
- [DM-RST è®ºæ–‡](https://...)
- [ModernBERT æ¨¡å‹](https://huggingface.co/...)

## ğŸ“§ è”ç³»æ–¹å¼

æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Ÿè¯·è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

---

**æœ€åæ›´æ–°**: 2024å¹´
**ç‰ˆæœ¬**: 1.0
