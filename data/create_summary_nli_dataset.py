#!/usr/bin/env python3
"""
åˆ›å»ºSummaryç±»åˆ«çš„NLIå¹»è§‰æ£€æµ‹æ•°æ®é›†
è¾“å‡ºæ ¼å¼ï¼šxlsx
å­—æ®µï¼šid, context, output, label, split, task_type
"""

import pandas as pd
import json
import ast
from pathlib import Path

def load_summary_data():
    """åŠ è½½Summaryç±»åˆ«çš„æ•°æ®"""
    # ä»ŽåŽŸå§‹parquetæ–‡ä»¶åŠ è½½å¹¶ç­›é€‰Summaryæ•°æ®
    train_df = pd.read_parquet("train-00000-of-00001.parquet")
    test_df = pd.read_parquet("test-00000-of-00001.parquet")
    
    # ç­›é€‰Summaryç±»åˆ«
    train_summary = train_df[train_df['task_type'] == 'Summary'].copy()
    test_summary = test_df[test_df['task_type'] == 'Summary'].copy()
    
    # æ·»åŠ splitæ ‡è¯†
    train_summary['split'] = 'train'
    test_summary['split'] = 'test'
    
    # åˆå¹¶
    summary_df = pd.concat([train_summary, test_summary], ignore_index=True)
    
    return summary_df

def process_hallucination_labels(hallucination_labels):
    """
    å¤„ç†å¹»è§‰æ ‡ç­¾ï¼Œå¦‚æžœä»»ä½•æ ‡ç­¾ä¸­æœ‰1å°±è®¤ä¸ºæ˜¯å¹»è§‰æ•°æ®
    è¿”å›žï¼š1è¡¨ç¤ºæœ‰å¹»è§‰ï¼Œ0è¡¨ç¤ºæ— å¹»è§‰
    """
    try:
        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„å­—å…¸
        if isinstance(hallucination_labels, str):
            labels_dict = ast.literal_eval(hallucination_labels)
        elif isinstance(hallucination_labels, dict):
            labels_dict = hallucination_labels
        else:
            return 0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ ‡ç­¾å€¼ä¸º1æˆ–å¤§äºŽ0
        for key, value in labels_dict.items():
            if isinstance(value, (int, float)) and value > 0:
                return 1
        
        return 0
    except:
        # å¦‚æžœè§£æžå¤±è´¥ï¼Œé»˜è®¤ä¸º0ï¼ˆæ— å¹»è§‰ï¼‰
        return 0

def create_nli_dataset(summary_df):
    """åˆ›å»ºNLIæ•°æ®é›†"""
    nli_data = []
    
    for idx, row in summary_df.iterrows():
        # åˆ›å»ºå”¯ä¸€ID
        nli_id = f"summary_{row['split']}_{idx}"
        
        # å¤„ç†å¹»è§‰æ ‡ç­¾
        label = process_hallucination_labels(row['hallucination_labels_processed'])
        
        nli_record = {
            'id': nli_id,
            'context': row['context'],
            'output': row['output'],
            'label': label,
            'split': row['split'],
            'task_type': row['task_type']
        }
        
        nli_data.append(nli_record)
    
    return pd.DataFrame(nli_data)

def analyze_dataset_statistics(nli_df):
    """åˆ†æžæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'total_samples': len(nli_df),
        'hallucination_samples': len(nli_df[nli_df['label'] == 1]),
        'no_hallucination_samples': len(nli_df[nli_df['label'] == 0]),
        'hallucination_rate': len(nli_df[nli_df['label'] == 1]) / len(nli_df) * 100,
        'train_samples': len(nli_df[nli_df['split'] == 'train']),
        'test_samples': len(nli_df[nli_df['split'] == 'test']),
        'train_hallucination_rate': len(nli_df[(nli_df['split'] == 'train') & (nli_df['label'] == 1)]) / len(nli_df[nli_df['split'] == 'train']) * 100,
        'test_hallucination_rate': len(nli_df[(nli_df['split'] == 'test') & (nli_df['label'] == 1)]) / len(nli_df[nli_df['split'] == 'test']) * 100
    }
    return stats

def save_nli_dataset(nli_df, stats):
    """ä¿å­˜NLIæ•°æ®é›†ä¸ºxlsxæ ¼å¼"""
    filename = "summary_nli_hallucination_dataset.xlsx"
    
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        # ä¸»æ•°æ®é›†
        nli_df.to_excel(writer, sheet_name='NLIæ•°æ®é›†', index=False)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats_data = {
            'ç»Ÿè®¡æŒ‡æ ‡': [
                'æ€»æ ·æœ¬æ•°', 
                'å¹»è§‰æ ·æœ¬æ•°', 
                'éžå¹»è§‰æ ·æœ¬æ•°', 
                'å¹»è§‰çŽ‡(%)', 
                'è®­ç»ƒé›†æ ·æœ¬æ•°', 
                'æµ‹è¯•é›†æ ·æœ¬æ•°',
                'è®­ç»ƒé›†å¹»è§‰çŽ‡(%)',
                'æµ‹è¯•é›†å¹»è§‰çŽ‡(%)'
            ],
            'æ•°å€¼': [
                stats['total_samples'],
                stats['hallucination_samples'],
                stats['no_hallucination_samples'],
                f"{stats['hallucination_rate']:.2f}%",
                stats['train_samples'],
                stats['test_samples'],
                f"{stats['train_hallucination_rate']:.2f}%",
                f"{stats['test_hallucination_rate']:.2f}%"
            ]
        }
        stats_df = pd.DataFrame(stats_data)
        stats_df.to_excel(writer, sheet_name='æ•°æ®é›†ç»Ÿè®¡', index=False)
        
        # æŒ‰splitåˆ†ç»„çš„æ•°æ®
        train_data = nli_df[nli_df['split'] == 'train']
        test_data = nli_df[nli_df['split'] == 'test']
        
        train_data.to_excel(writer, sheet_name='è®­ç»ƒé›†', index=False)
        test_data.to_excel(writer, sheet_name='æµ‹è¯•é›†', index=False)
        
        # æ ‡ç­¾åˆ†å¸ƒåˆ†æž
        label_stats = []
        for split in ['train', 'test']:
            split_data = nli_df[nli_df['split'] == split]
            label_counts = split_data['label'].value_counts().sort_index()
            
            for label, count in label_counts.items():
                percentage = count / len(split_data) * 100
                label_name = "æœ‰å¹»è§‰" if label == 1 else "æ— å¹»è§‰"
                label_stats.append({
                    'æ•°æ®é›†': split,
                    'æ ‡ç­¾': label_name,
                    'æ ‡ç­¾å€¼': label,
                    'æ•°é‡': count,
                    'ç™¾åˆ†æ¯”': f"{percentage:.2f}%"
                })
        
        label_df = pd.DataFrame(label_stats)
        label_df.to_excel(writer, sheet_name='æ ‡ç­¾åˆ†å¸ƒ', index=False)
    
    return filename

def main():
    """ä¸»å‡½æ•°"""
    print("ðŸ”„ æ­£åœ¨åŠ è½½Summaryç±»åˆ«æ•°æ®...")
    summary_df = load_summary_data()
    
    print(f"ðŸ“Š Summaryæ•°æ®åŠ è½½å®Œæˆ: æ€»æ ·æœ¬ {len(summary_df)}")
    print(f"   è®­ç»ƒé›†: {len(summary_df[summary_df['split'] == 'train'])}")
    print(f"   æµ‹è¯•é›†: {len(summary_df[summary_df['split'] == 'test'])}")
    
    print("ðŸ”„ æ­£åœ¨åˆ›å»ºNLIå¹»è§‰æ£€æµ‹æ•°æ®é›†...")
    nli_df = create_nli_dataset(summary_df)
    
    print("ðŸ”„ æ­£åœ¨åˆ†æžæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
    stats = analyze_dataset_statistics(nli_df)
    
    print("ðŸ”„ æ­£åœ¨ä¿å­˜æ•°æ®é›†...")
    filename = save_nli_dataset(nli_df, stats)
    
    print("\nâœ… NLIå¹»è§‰æ£€æµ‹æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    print(f"ðŸ“ ä¿å­˜æ–‡ä»¶: {filename}")
    
    print(f"\nðŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"   å¹»è§‰æ ·æœ¬æ•°: {stats['hallucination_samples']} ({stats['hallucination_rate']:.2f}%)")
    print(f"   éžå¹»è§‰æ ·æœ¬æ•°: {stats['no_hallucination_samples']} ({100-stats['hallucination_rate']:.2f}%)")
    print(f"   è®­ç»ƒé›†å¹»è§‰çŽ‡: {stats['train_hallucination_rate']:.2f}%")
    print(f"   æµ‹è¯•é›†å¹»è§‰çŽ‡: {stats['test_hallucination_rate']:.2f}%")
    
    # æ˜¾ç¤ºæ ·æœ¬é¢„è§ˆ
    print(f"\nðŸ“‹ æ•°æ®é›†é¢„è§ˆ:")
    print("å­—æ®µåç§°: id, context, output, label, split, task_type")
    print("æ ‡ç­¾è¯´æ˜Ž: 0=æ— å¹»è§‰, 1=æœ‰å¹»è§‰")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„åŸºæœ¬ä¿¡æ¯
    print(f"\næ ·æœ¬ç¤ºä¾‹:")
    for i in range(min(3, len(nli_df))):
        row = nli_df.iloc[i]
        context_preview = row['context'][:100] + "..." if len(row['context']) > 100 else row['context']
        output_preview = row['output'][:100] + "..." if len(row['output']) > 100 else row['output']
        print(f"  ID: {row['id']}")
        print(f"  Context: {context_preview}")
        print(f"  Output: {output_preview}")
        print(f"  Label: {row['label']} ({'æœ‰å¹»è§‰' if row['label'] == 1 else 'æ— å¹»è§‰'})")
        print(f"  Split: {row['split']}")
        print("  ---")

if __name__ == "__main__":
    main()