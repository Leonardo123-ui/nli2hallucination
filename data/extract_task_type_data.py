#!/usr/bin/env python3
"""
æå–ä¸åŒtask_typeçš„å…·ä½“æ•°æ®å¹¶åˆ†åˆ«ä¿å­˜
è¾“å‡ºæ ¼å¼ï¼šCSVï¼ˆä¸»è¦å­—æ®µï¼‰å’ŒJSONï¼ˆå®Œæ•´æ•°æ®ï¼‰
"""

import pandas as pd
import json
import os
from pathlib import Path

def load_data():
    """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
    train_path = "train-00000-of-00001.parquet"
    test_path = "test-00000-of-00001.parquet"
    
    train_df = pd.read_parquet(train_path)
    test_df = pd.read_parquet(test_path)
    
    # æ·»åŠ æ•°æ®é›†æ ‡è¯†
    train_df['dataset'] = 'train'
    test_df['dataset'] = 'test'
    
    combined_df = pd.concat([train_df, test_df], ignore_index=True)
    return combined_df

def extract_task_type_data(df, task_type):
    """æå–ç‰¹å®štask_typeçš„æ•°æ®"""
    return df[df['task_type'] == task_type].copy()

def save_csv_format(task_data, task_type):
    """ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä¸»è¦å­—æ®µï¼‰"""
    # é€‰æ‹©ä¸»è¦å­—æ®µç”¨äºCSVå¯¼å‡º
    csv_columns = [
        'dataset', 'task_type', 'model', 'temperature', 'quality',
        'query', 'context', 'output', 'hallucination_labels_processed'
    ]
    
    # å¤„ç†å¤æ‚å­—æ®µ
    csv_data = task_data[csv_columns].copy()
    
    # å°†hallucination_labels_processedè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    csv_data['hallucination_labels_processed'] = csv_data['hallucination_labels_processed'].astype(str)
    
    filename = f"{task_type.lower()}_data.csv"
    csv_data.to_csv(filename, index=False, encoding='utf-8')
    return filename

def save_json_format(task_data, task_type):
    """ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆå®Œæ•´æ•°æ®ï¼‰"""
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    records = task_data.to_dict('records')
    
    filename = f"{task_type.lower()}_data.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(records, f, ensure_ascii=False, indent=2, default=str)
    return filename

def save_xlsx_format(task_data, task_type):
    """ä¿å­˜ä¸ºExcelæ ¼å¼"""
    try:
        # å¤„ç†å¤æ‚å­—æ®µ
        xlsx_data = task_data.copy()
        xlsx_data['hallucination_labels_processed'] = xlsx_data['hallucination_labels_processed'].astype(str)
        
        filename = f"{task_type.lower()}_data.xlsx"
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # ä¸»æ•°æ®sheet
            xlsx_data.to_excel(writer, sheet_name='å®Œæ•´æ•°æ®', index=False)
            
            # ç»Ÿè®¡æ‘˜è¦sheet
            summary_data = {
                'æŒ‡æ ‡': ['æ€»æ ·æœ¬æ•°', 'è®­ç»ƒæ ·æœ¬', 'æµ‹è¯•æ ·æœ¬', 'å¥½è´¨é‡æ ·æœ¬', 'å¥½è´¨é‡ç™¾åˆ†æ¯”'],
                'å€¼': [
                    len(task_data),
                    len(task_data[task_data['dataset'] == 'train']),
                    len(task_data[task_data['dataset'] == 'test']),
                    len(task_data[task_data['quality'] == 'good']),
                    f"{len(task_data[task_data['quality'] == 'good']) / len(task_data) * 100:.2f}%"
                ]
            }
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='ç»Ÿè®¡æ‘˜è¦', index=False)
            
        return filename
    except ImportError:
        print(f"âš ï¸  éœ€è¦å®‰è£… openpyxl æ‰èƒ½å¯¼å‡º {task_type} çš„ XLSX æ–‡ä»¶")
        return None

def create_summary_report(task_counts):
    """åˆ›å»ºæå–æ‘˜è¦æŠ¥å‘Š"""
    report = {
        'extraction_summary': {
            'total_task_types': len(task_counts),
            'task_type_counts': task_counts,
            'files_generated': []
        }
    }
    
    with open('extraction_summary.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®...")
    df = load_data()
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: æ€»æ ·æœ¬ {len(df)}")
    
    # è·å–æ‰€æœ‰task_type
    task_types = df['task_type'].unique()
    print(f"ğŸ“‹ å‘ç°çš„task_type: {list(task_types)}")
    
    task_counts = {}
    generated_files = []
    
    # ä¸ºæ¯ä¸ªtask_typeæå–å¹¶ä¿å­˜æ•°æ®
    for task_type in task_types:
        print(f"\nğŸ”„ æ­£åœ¨å¤„ç† {task_type}...")
        
        # æå–æ•°æ®
        task_data = extract_task_type_data(df, task_type)
        task_counts[task_type] = len(task_data)
        
        print(f"   æ ·æœ¬æ•°é‡: {len(task_data)}")
        print(f"   è®­ç»ƒé›†: {len(task_data[task_data['dataset'] == 'train'])}")
        print(f"   æµ‹è¯•é›†: {len(task_data[task_data['dataset'] == 'test'])}")
        
        # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
        csv_file = save_csv_format(task_data, task_type)
        json_file = save_json_format(task_data, task_type)
        xlsx_file = save_xlsx_format(task_data, task_type)
        
        generated_files.extend([csv_file, json_file])
        if xlsx_file:
            generated_files.append(xlsx_file)
        
        print(f"   âœ… å·²ä¿å­˜: {csv_file}, {json_file}" + (f", {xlsx_file}" if xlsx_file else ""))
    
    # åˆ›å»ºæ‘˜è¦æŠ¥å‘Š
    create_summary_report(task_counts)
    
    print(f"\nâœ… æ•°æ®æå–å®Œæˆï¼")
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for file in generated_files:
        if os.path.exists(file):
            size_mb = os.path.getsize(file) / (1024 * 1024)
            print(f"   - {file} ({size_mb:.2f} MB)")
    
    print(f"\nğŸ“Š å„task_typeæ ·æœ¬ç»Ÿè®¡:")
    for task_type, count in task_counts.items():
        percentage = count / len(df) * 100
        print(f"   - {task_type}: {count} æ ·æœ¬ ({percentage:.2f}%)")

if __name__ == "__main__":
    main()