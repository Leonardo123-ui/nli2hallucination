# ğŸ¯ LLMæ¨ç† - å¿«é€Ÿå‚è€ƒå¡

## âš¡ æœ€å¿«å¼€å§‹ï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# 1ï¸âƒ£ å¯åŠ¨Ollamaï¼ˆä¸€ä¸ªç»ˆç«¯ï¼‰
ollama serve

# 2ï¸âƒ£ æ‹‰å–æ¨¡å‹ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
ollama pull llama3
ollama pull qwen:7b

# 3ï¸âƒ£ å¿«é€Ÿæ¨ç†
cd /mnt/nlp/yuanmengying/nli2hallucination/data/bert-classifier
python llm_inference.py --model_name llama3 --sample_size 50

# 4ï¸âƒ£ æŸ¥çœ‹ç»“æœ
cat ./llm_results/llm_results.json
```

---

## ğŸ“‹ å¸¸ç”¨å‘½ä»¤

### åŸºç¡€æ¨ç†

```bash
# llama3ï¼ˆè‹±æ–‡ï¼Œæ¨èï¼‰
python llm_inference.py --model_name llama3

# qwen 7Bï¼ˆä¸­æ–‡ï¼‰
python llm_inference.py --model_name qwen:7b --use_zh_prompt

# qwen 14Bï¼ˆæ›´å¥½çš„è´¨é‡ï¼‰
python llm_inference.py --model_name qwen:14b --use_zh_prompt
```

### é‡‡æ ·å’Œé™åˆ¶

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ50ä¸ªæ ·æœ¬ï¼‰
python llm_inference.py --model_name llama3 --sample_size 50

# ä¸­ç­‰æµ‹è¯•ï¼ˆ200ä¸ªæ ·æœ¬ï¼‰
python llm_inference.py --model_name llama3 --sample_size 200

# å®Œæ•´æµ‹è¯•ï¼ˆæ‰€æœ‰900ä¸ªæ ·æœ¬ï¼‰
python llm_inference.py --model_name llama3
```

### å¯¹æ¯”åˆ†æ

```bash
# å¯¹æ¯”å¤šä¸ªæ¨¡å‹
python config_examples.py compare

# BERT vs LLMå¯¹æ¯”
python compare_models.py

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
python config_examples.py report
```

### ç¯å¢ƒæ£€æŸ¥

```bash
# æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å°±ç»ª
python check_environment.py
```

---

## ğŸ“‚ è¾“å‡ºä½ç½®

```bash
# æ¨ç†ç»“æœ
./llm_results/
  â””â”€â”€ llm_results.json              # è¯„ä¼°æŒ‡æ ‡
  â””â”€â”€ llm_detailed_predictions.xlsx # è¯¦ç»†é¢„æµ‹

# å¯¹æ¯”ç»“æœ
./comparison_results/
  â””â”€â”€ bert_llm_comparison.xlsx      # å¯¹æ¯”ç»“æœ
  â””â”€â”€ disagreement_cases.xlsx       # åˆ†æ­§æ¡ˆä¾‹
```

---

## ğŸ”§ å‚æ•°é€ŸæŸ¥

| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `--model_name` | æ¨¡å‹åç§° | `llama3`, `qwen:7b` |
| `--deploy_type` | éƒ¨ç½²æ–¹å¼ | `ollama`, `huggingface` |
| `--sample_size` | é‡‡æ ·å¤§å° | `50`, `100`, `None` |
| `--use_zh_prompt` | ä¸­æ–‡æç¤º | (å­˜åœ¨=True) |
| `--output_dir` | è¾“å‡ºç›®å½• | `./results` |

---

## ğŸ” æŸ¥çœ‹ç»“æœ

```python
# PythonæŸ¥çœ‹
import json, pandas as pd

# æŸ¥çœ‹æŒ‡æ ‡
with open('./llm_results/llm_results.json') as f:
    metrics = json.load(f)['detailed_metrics']
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
    print(f"å¹»è§‰F1: {metrics['hallucination']['f1_score']:.4f}")

# æŸ¥çœ‹è¯¦ç»†é¢„æµ‹
df = pd.read_excel('./llm_results/llm_detailed_predictions.xlsx')
print(df[['id', 'label', 'llm_prediction', 'correct_prediction']])
```

---

## âŒ é‡åˆ°é—®é¢˜ï¼Ÿ

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|--------|
| Ollamaè¿æ¥å¤±è´¥ | `ollama serve` å¯åŠ¨æœåŠ¡ |
| æ˜¾å­˜ä¸è¶³ | ç”¨æ›´å°æ¨¡å‹ï¼ˆ7Bï¼‰æˆ–å‡å°‘æ ·æœ¬ |
| æ¨ç†å¾ˆæ…¢ | æ£€æŸ¥GPUï¼š`python -c "import torch; print(torch.cuda.is_available())"` |
| æ¨¡å‹æœªæ‰¾åˆ° | `ollama list` æŸ¥çœ‹ï¼Œ`ollama pull llama3` å®‰è£… |

---

## ğŸ“– è¯¦ç»†æ–‡æ¡£

| æ–‡ä»¶ | å†…å®¹ |
|------|------|
| `QUICK_START.md` | 5åˆ†é’Ÿä¸Šæ‰‹æŒ‡å— |
| `LLM_INFERENCE_GUIDE.md` | å®Œæ•´åŠŸèƒ½è¯´æ˜ |
| `config_examples.py` | 8ä¸ªä»£ç ç¤ºä¾‹ |
| `PROJECT_SUMMARY.md` | é¡¹ç›®æ€»ä½“è¯´æ˜ |

---

## ğŸ’» ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- ç¡¬ç›˜ç©ºé—´ï¼š5-10GBï¼ˆç”¨äºæ¨¡å‹ï¼‰
- GPUæ˜¾å­˜ï¼š4GB+ï¼ˆä½¿ç”¨Ollamaï¼‰æˆ– 8GB+ï¼ˆHuggingFaceï¼‰

---

## ğŸš€ æ¨èæµç¨‹

```
Day 1: å¿«é€ŸéªŒè¯
  â””â”€ python llm_inference.py --sample_size 50

Day 2-3: å®Œæ•´è¯„ä¼°
  â””â”€ python llm_inference.py

Day 4-5: æ¨¡å‹å¯¹æ¯”
  â””â”€ python config_examples.py compare
  â””â”€ python compare_models.py

Day 6+: ä¼˜åŒ–å’Œé›†æˆ
  â””â”€ è°ƒæ•´Promptæˆ–å°è¯•æ›´å¤§æ¨¡å‹
  â””â”€ é›†æˆåˆ°ç”Ÿäº§æµç¨‹
```

---

## ğŸ‰ ç«‹å³å¼€å§‹ï¼

```bash
# æ£€æŸ¥ç¯å¢ƒ
python check_environment.py

# å¿«é€Ÿæµ‹è¯•
python llm_inference.py --model_name llama3 --sample_size 50

# æŸ¥çœ‹å¸®åŠ©
python llm_inference.py --help
```

---

**æ›´å¤šå¸®åŠ©**: æŸ¥çœ‹ `QUICK_START.md` æˆ– `LLM_INFERENCE_GUIDE.md`
