# LLM å¹»è§‰æ£€æµ‹æ¨ç†æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨ `llm_inference.py` è„šæœ¬ï¼Œé€šè¿‡ llama3 å’Œ qwen3 ç­‰å¤§è¯­è¨€æ¨¡å‹å¯¹å¹»è§‰æ£€æµ‹æ•°æ®è¿›è¡Œæ¨ç†ã€‚

## ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [éƒ¨ç½²æ–¹å¼](#éƒ¨ç½²æ–¹å¼)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [è¯¦ç»†ç”¨æ³•](#è¯¦ç»†ç”¨æ³•)
5. [è¾“å‡ºç»“æœ](#è¾“å‡ºç»“æœ)
6. [å¯¹æ¯”åˆ†æ](#å¯¹æ¯”åˆ†æ)

---

## ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–åŒ…

```bash
pip install torch transformers pandas numpy scikit-learn tqdm requests

# å¦‚æœä½¿ç”¨APIæ–¹å¼ï¼Œè¿˜éœ€è¦å®‰è£…å¯¹åº”çš„åº“
# é˜¿é‡Œäº‘Qwen API
pip install dashscope

# OpenAI APIï¼ˆæ”¯æŒllamaï¼‰
pip install openai
```

### ç¡¬ä»¶è¦æ±‚

| éƒ¨ç½²æ–¹å¼ | æ˜¾å­˜è¦æ±‚ | CPUå†…å­˜è¦æ±‚ |
|---------|---------|----------|
| Ollamaï¼ˆæœ¬åœ°ï¼‰ | 4-8GB | 8GB+ |
| HuggingFaceåŠ è½½ | 8-16GB | 16GB+ |
| APIè°ƒç”¨ | æ—  | ä½ |

---

## éƒ¨ç½²æ–¹å¼

### æ–¹å¼1ï¼šä½¿ç”¨Ollamaï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰

#### 1.1 å®‰è£…Ollama

```bash
# è®¿é—®å®˜ç½‘ä¸‹è½½å®‰è£…
https://ollama.ai

# æˆ–ä½¿ç”¨åŒ…ç®¡ç†å™¨ï¼ˆLinuxï¼‰
curl https://ollama.ai/install.sh | sh
```

#### 1.2 å¯åŠ¨OllamaæœåŠ¡

```bash
# é»˜è®¤åœ¨localhost:11434
ollama serve

# æˆ–åœ¨åå°è¿è¡Œ
nohup ollama serve > ollama.log 2>&1 &
```

#### 1.3 æ‹‰å–æ¨¡å‹

```bash
# æ‹‰å–llama3
ollama pull llama3

# æ‹‰å–qwenï¼ˆéœ€è¦Ollama 0.1.23+ï¼‰
ollama pull qwen:7b
ollama pull qwen:14b

# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list
```

#### 1.4 è¿è¡Œæ¨ç†

```bash
cd /mnt/nlp/yuanmengying/nli2hallucination/data/bert-classifier

# ä½¿ç”¨llama3
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./llm_results/llama3 \
  --sample_size 50  # å…ˆç”¨50ä¸ªæ ·æœ¬æµ‹è¯•

# ä½¿ç”¨qwen
python llm_inference.py \
  --model_name qwen:7b \
  --deploy_type ollama \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./llm_results/qwen \
  --sample_size 50
```

---

### æ–¹å¼2ï¼šä½¿ç”¨HuggingFaceç›´æ¥åŠ è½½

#### 2.1 ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½llama3ï¼ˆéœ€è¦HuggingFace Access Tokenï¼‰
huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir ./models/llama3

# æˆ–ä½¿ç”¨Python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_id, token="your_hf_token")
model = AutoModelForCausalLM.from_pretrained(model_id, token="your_hf_token")

model.save_pretrained("./models/llama3")
tokenizer.save_pretrained("./models/llama3")
```

#### 2.2 è¿è¡Œæ¨ç†

```bash
# ä½¿ç”¨æœ¬åœ°HuggingFaceæ¨¡å‹
python llm_inference.py \
  --model_name llama3 \
  --deploy_type huggingface \
  --model_path ./models/llama3 \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./llm_results/llama3 \
  --sample_size 100
```

---

### æ–¹å¼3ï¼šä½¿ç”¨APIè°ƒç”¨

#### 3.1 é˜¿é‡Œäº‘Qwen API

```bash
# å®‰è£…dashscopeåº“
pip install dashscope

# è®¾ç½®APIå¯†é’¥
export DASHSCOPE_API_KEY="your_api_key"

# è¿è¡Œæ¨ç†
python llm_inference.py \
  --model_name qwen \
  --deploy_type api \
  --api_url https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation \
  --api_key your_api_key \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./llm_results/qwen_api
```

#### 3.2 OpenAIå…¼å®¹APIï¼ˆå¦‚Togetherã€Replicateç­‰ï¼‰

```bash
# å®‰è£…openaiåº“
pip install openai

# è¿è¡Œæ¨ç†
python llm_inference.py \
  --model_name llama-2-70b \
  --deploy_type api \
  --api_key your_api_key \
  --api_url https://api.together.xyz \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./llm_results/llama3_api
```

---

## å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„å¼€å§‹æ–¹å¼ï¼šä½¿ç”¨Ollama

```bash
# ç¬¬1æ­¥ï¼šå®‰è£…å¹¶å¯åŠ¨Ollamaï¼ˆä¸€æ¬¡æ€§ï¼‰
curl https://ollama.ai/install.sh | sh
ollama serve

# ç¬¬2æ­¥ï¼šåœ¨å¦ä¸€ä¸ªç»ˆç«¯æ‹‰å–æ¨¡å‹ï¼ˆä¸€æ¬¡æ€§ï¼‰
ollama pull llama3
ollama pull qwen:7b

# ç¬¬3æ­¥ï¼šè¿è¡Œæ¨ç†
cd /mnt/nlp/yuanmengying/nli2hallucination/data/bert-classifier

# ä½¿ç”¨50ä¸ªæ ·æœ¬å¿«é€Ÿæµ‹è¯•
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --output_dir ./llm_results/llama3_test \
  --sample_size 50

# ä½¿ç”¨å…¨éƒ¨900ä¸ªæµ‹è¯•æ ·æœ¬
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --output_dir ./llm_results/llama3_full
```

### è¿è¡Œå¤šä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”

```bash
# åˆ›å»ºå¯¹æ¯”ç›®å½•
mkdir -p comparison_results

# æµ‹è¯•llama3
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --output_dir ./comparison_results/llama3 \
  --sample_size 100

# æµ‹è¯•qwen
python llm_inference.py \
  --model_name qwen:7b \
  --deploy_type ollama \
  --output_dir ./comparison_results/qwen7b \
  --sample_size 100

# æµ‹è¯•qwen 14b
python llm_inference.py \
  --model_name qwen:14b \
  --deploy_type ollama \
  --output_dir ./comparison_results/qwen14b \
  --sample_size 100
```

---

## è¯¦ç»†ç”¨æ³•

### å‘½ä»¤è¡Œå‚æ•°

#### æ¨¡å‹é…ç½®å‚æ•°

| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `--model_name` | æ¨¡å‹åç§°ï¼ˆå¿…éœ€ï¼‰ | `llama3`, `qwen:7b`, `qwen:14b` |
| `--deploy_type` | éƒ¨ç½²æ–¹å¼ | `ollama`, `huggingface`, `api` |
| `--model_path` | HuggingFaceæ¨¡å‹è·¯å¾„ | `./models/llama3` |
| `--ollama_url` | OllamaæœåŠ¡åœ°å€ | `http://localhost:11434` |
| `--api_key` | APIå¯†é’¥ | `sk-xxx...` |
| `--api_url` | APIåœ°å€ | `https://api.together.xyz` |

#### æ•°æ®é…ç½®å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|-------|
| `--data_path` | æµ‹è¯•æ•°æ®è·¯å¾„ | `../summary_nli_hallucination_dataset.xlsx` |
| `--sample_size` | æŠ½æ ·å¤§å° | `None`ï¼ˆä½¿ç”¨å…¨éƒ¨ï¼‰ |
| `--output_dir` | ç»“æœä¿å­˜ç›®å½• | `./llm_results` |

#### æ¨ç†é…ç½®å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|-------|
| `--use_zh_prompt` | ä½¿ç”¨ä¸­æ–‡æç¤ºè¯ | `False` |
| `--temperature` | é‡‡æ ·æ¸©åº¦ï¼ˆ0=ç¡®å®šæ€§ï¼‰ | `0.0` |
| `--max_tokens` | æœ€å¤§ç”Ÿæˆtokenæ•° | `100` |

### å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

#### ç¤ºä¾‹1ï¼šä½¿ç”¨llama3è¿›è¡Œæ¨ç†ï¼ˆè‹±æ–‡æç¤ºè¯ï¼‰

```bash
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./results/llama3_en \
  --sample_size 200 \
  --temperature 0.0 \
  --max_tokens 50
```

#### ç¤ºä¾‹2ï¼šä½¿ç”¨qwenè¿›è¡Œæ¨ç†ï¼ˆä¸­æ–‡æç¤ºè¯ï¼‰

```bash
python llm_inference.py \
  --model_name qwen:7b \
  --deploy_type ollama \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./results/qwen_zh \
  --use_zh_prompt \
  --sample_size 200
```

#### ç¤ºä¾‹3ï¼šä½¿ç”¨HuggingFaceæ¨¡å‹è¿›è¡Œæ¨ç†

```bash
python llm_inference.py \
  --model_name llama3 \
  --deploy_type huggingface \
  --model_path /path/to/local/llama3 \
  --data_path ../summary_nli_hallucination_dataset.xlsx \
  --output_dir ./results/llama3_hf \
  --sample_size 100
```

#### ç¤ºä¾‹4ï¼šä½¿ç”¨APIè¿›è¡Œæ¨ç†ï¼ˆé˜¿é‡Œäº‘Qwenï¼‰

```bash
python llm_inference.py \
  --model_name qwen \
  --deploy_type api \
  --api_key "sk-xxx..." \
  --api_url https://dashscope.aliyuncs.com \
  --output_dir ./results/qwen_api \
  --sample_size 100
```

---

## è¾“å‡ºç»“æœ

### è¾“å‡ºæ–‡ä»¶è¯´æ˜

è¿è¡Œæ¨ç†åï¼Œä¼šåœ¨æŒ‡å®šçš„ `--output_dir` ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

#### 1. `llm_results.json` - æ¨ç†ç»“æœæ€»ç»“

```json
{
  "model_name": "llama3",
  "deploy_type": "ollama",
  "inference_time": "2024-01-15T10:30:00",
  "test_size": 900,
  "valid_predictions": 890,
  "use_zh_prompt": false,
  "detailed_metrics": {
    "accuracy": 0.7651,
    "macro_precision": 0.6234,
    "macro_recall": 0.6123,
    "macro_f1": 0.6178,
    "no_hallucination": {
      "precision": 0.8234,
      "recall": 0.8567,
      "f1_score": 0.8398,
      "support": 630
    },
    "hallucination": {
      "precision": 0.5145,
      "recall": 0.4234,
      "f1_score": 0.4646,
      "support": 270
    },
    "confusion_matrix": {
      "true_negatives": 540,
      "false_positives": 90,
      "false_negatives": 156,
      "true_positives": 104
    },
    "specificity": 0.857,
    "sensitivity": 0.4
  }
}
```

#### 2. `llm_detailed_predictions.xlsx` - è¯¦ç»†é¢„æµ‹ç»“æœ

åŒ…å«ä»¥ä¸‹åˆ—ï¼š
- `id`: æ ·æœ¬ID
- `context`: ä¸Šä¸‹æ–‡
- `output`: ç”Ÿæˆçš„æ–‡æœ¬
- `label`: çœŸå®æ ‡ç­¾ï¼ˆ0=æ— å¹»è§‰ï¼Œ1=æœ‰å¹»è§‰ï¼‰
- `llm_prediction`: LLMé¢„æµ‹æ ‡ç­¾
- `llm_confidence`: LLMé¢„æµ‹ç½®ä¿¡åº¦
- `llm_raw_output`: LLMåŸå§‹è¾“å‡º
- `correct_prediction`: é¢„æµ‹æ˜¯å¦æ­£ç¡®

### ç»“æœè§£è¯»

æ¨ç†å®Œæˆåï¼Œä¼šåœ¨ç»ˆç«¯è¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š

```
======================================================================
LLAMA3 å¹»è§‰æ£€æµ‹æ¨ç†ç»“æœ
======================================================================

ğŸ“Š æ€»ä½“æ€§èƒ½æŒ‡æ ‡:
å‡†ç¡®ç‡ (Accuracy): 0.7651
å®å¹³å‡ç²¾ç¡®ç‡: 0.6234
å®å¹³å‡å¬å›ç‡: 0.6123
å®å¹³å‡F1åˆ†æ•°: 0.6178

ğŸ” æ— å¹»è§‰ç±»åˆ« (æ ‡ç­¾0):
ç²¾ç¡®ç‡: 0.8234, å¬å›ç‡: 0.8567, F1: 0.8398

âš ï¸  æœ‰å¹»è§‰ç±»åˆ« (æ ‡ç­¾1):
ç²¾ç¡®ç‡: 0.5145, å¬å›ç‡: 0.4234, F1: 0.4646

ğŸ“ˆ å…³é”®æŒ‡æ ‡:
æ•æ„Ÿæ€§ (Sensitivity): 0.4000
ç‰¹å¼‚æ€§ (Specificity): 0.8571

ğŸ”¢ æ··æ·†çŸ©é˜µ:
çœŸé˜´æ€§ (TN): 540, å‡é˜³æ€§ (FP): 90
å‡é˜´æ€§ (FN): 156, çœŸé˜³æ€§ (TP): 104

ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: ./llm_results/llama3
======================================================================
```

---

## å¯¹æ¯”åˆ†æ

### å¯¹æ¯”BERTå’ŒLLMç»“æœ

```python
import pandas as pd
import json

# åŠ è½½BERTç»“æœ
bert_results = pd.read_excel('./test_results/detailed_predictions.xlsx')

# åŠ è½½LLMç»“æœ
llm_results = pd.read_excel('./llm_results/llama3/llm_detailed_predictions.xlsx')

# åˆå¹¶ç»“æœ
comparison = bert_results.merge(
    llm_results[['id', 'llm_prediction', 'llm_confidence']],
    on='id',
    how='inner'
)

# è®¡ç®—ä¸€è‡´æ€§
agreement = (comparison['predicted_label'] == comparison['llm_prediction']).mean()
print(f"BERTå’ŒLLMé¢„æµ‹ä¸€è‡´ç‡: {agreement:.2%}")

# ç»Ÿè®¡åˆ†æ­§æƒ…å†µ
disagreement = comparison[comparison['predicted_label'] != comparison['llm_prediction']]
print(f"æ€»åˆ†æ­§æ•°: {len(disagreement)}")

# ä¿å­˜å¯¹æ¯”ç»“æœ
comparison.to_excel('./comparison_bert_llm.xlsx', index=False)
```

### Pythonè„šæœ¬å¯¹æ¯”å¤šä¸ªæ¨¡å‹

```python
import json
import pandas as pd
from pathlib import Path

def compare_models(results_dirs):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„ç»“æœ"""

    results = {}

    for model_dir in results_dirs:
        model_name = Path(model_dir).name

        # åŠ è½½ç»“æœ
        with open(f'{model_dir}/llm_results.json', 'r', encoding='utf-8') as f:
            results[model_name] = json.load(f)

    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    comparison_df = pd.DataFrame({
        model: {
            'Accuracy': result['detailed_metrics']['accuracy'],
            'Precision': result['detailed_metrics']['hallucination']['precision'],
            'Recall': result['detailed_metrics']['hallucination']['recall'],
            'F1': result['detailed_metrics']['hallucination']['f1_score'],
        }
        for model, result in results.items()
    }).T

    print(comparison_df.to_string())
    return comparison_df

# ä½¿ç”¨ç¤ºä¾‹
model_dirs = [
    './comparison_results/llama3',
    './comparison_results/qwen7b',
    './comparison_results/qwen14b',
]

comparison = compare_models(model_dirs)
```

---

## å¸¸è§é—®é¢˜

### Q1: Ollama è¿æ¥è¶…æ—¶

**é—®é¢˜**ï¼š`æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ http://localhost:11434`

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥Ollamaæ˜¯å¦å·²å¯åŠ¨
ollama serve

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:11434/api/tags

# å¦‚æœæ— æ³•å¯åŠ¨ï¼Œå°è¯•é‡æ–°å®‰è£…
ollama --version
```

### Q2: æ˜¾å­˜ä¸è¶³

**é—®é¢˜**ï¼š`CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨Ollamaï¼ˆè‡ªåŠ¨ä¼˜åŒ–å†…å­˜ï¼‰
2. å‡å° `--sample_size`
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ `qwen:7b` è€Œé `qwen:14b`ï¼‰
4. ä½¿ç”¨APIæ–¹å¼ï¼ˆä¸å ç”¨æœ¬åœ°æ˜¾å­˜ï¼‰

### Q3: æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢

**åŸå› å’Œä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

| åŸå›  | ä¼˜åŒ–æ–¹æ¡ˆ |
|------|--------|
| æ¨¡å‹å¤ªå¤§ | ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–å‡å°‘é‡‡æ ·å¤§å° |
| ç¡¬ä»¶é…ç½®ä½ | ä½¿ç”¨æ›´å¿«çš„ç¡¬ä»¶æˆ–APIæœåŠ¡ |
| ç½‘ç»œå»¶è¿Ÿï¼ˆAPIï¼‰ | é€‰æ‹©æ›´è¿‘çš„æœåŠ¡å™¨æˆ–æœ¬åœ°éƒ¨ç½² |

### Q4: LLMé¢„æµ‹å‡†ç¡®ç‡åä½

**å¯èƒ½åŸå› **ï¼š
- Promptè®¾è®¡ä¸ä½³ - ä¿®æ”¹ `HALLUCINATION_PROMPT_TEMPLATE` æˆ– `HALLUCINATION_PROMPT_ZH`
- æ¨¡å‹é€‰æ‹©ä¸å½“ - å°è¯•æ›´å¤§çš„æ¨¡å‹
- ä»»åŠ¡å¤æ‚åº¦ - æ•°æ®ä¸­çš„å¹»è§‰ç±»å‹å¤æ‚ï¼ŒLLMå¯èƒ½è¾ƒéš¾åˆ¤æ–­

**ä¼˜åŒ–å»ºè®®**ï¼š
```python
# ä¿®æ”¹promptä»¥è·å¾—æ›´å¥½çš„ç»“æœ
CUSTOM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æœ¬è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚
è¯·åˆ¤æ–­ä»¥ä¸‹ç”Ÿæˆæ–‡æœ¬ä¸­æ˜¯å¦å­˜åœ¨ä¸ä¸Šä¸‹æ–‡ä¸ç¬¦çš„å†…å®¹ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

ç”Ÿæˆæ–‡æœ¬ï¼š{output}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ£€æŸ¥ï¼š
1. æ˜¯å¦æœ‰äº‹å®é”™è¯¯
2. æ˜¯å¦æœ‰ä¿¡æ¯é—æ¼æˆ–æ·»åŠ 
3. æ˜¯å¦æœ‰é€»è¾‘çŸ›ç›¾

ç­”æ¡ˆï¼š"""
```

---

## æ€§èƒ½å‚è€ƒ

### ä¸åŒéƒ¨ç½²æ–¹å¼çš„æ€§èƒ½å¯¹æ¯”

| éƒ¨ç½²æ–¹å¼ | æ¨ç†é€Ÿåº¦ | æ˜¾å­˜å ç”¨ | æ˜“ç”¨æ€§ |
|---------|--------|--------|------|
| Ollama | ä¸­ç­‰ | 4-8GB | æœ€ç®€å• |
| HuggingFace | å¿« | 8-16GB | ä¸­ç­‰ |
| API | æ…¢ | 0 | æœ€ç®€å• |

### ä¸åŒæ¨¡å‹çš„æ€§èƒ½å‚è€ƒ

| æ¨¡å‹ | å‚æ•° | æ¨ç†é€Ÿåº¦ | è´¨é‡ | æ˜¾å­˜ |
|------|------|--------|------|------|
| llama3 | 8B | ä¸­ç­‰ | è‰¯å¥½ | 6-8GB |
| llama3 | 70B | æ…¢ | ä¼˜ç§€ | 40GB+ |
| qwen | 7B | å¿« | è‰¯å¥½ | 6GB |
| qwen | 14B | ä¸­ç­‰ | ä¼˜ç§€ | 12GB |
| qwen | 72B | æ…¢ | ä¼˜ç§€ | 40GB+ |

---

## æ‰©å±•å’Œè‡ªå®šä¹‰

### æ·»åŠ æ–°çš„LLMæ¨¡å‹

```python
# åœ¨ llm_inference.py ä¸­ä¿®æ”¹ LLMInference ç±»

def predict_custom_llm(self, context: str, output: str, use_zh: bool = False) -> Dict:
    """è‡ªå®šä¹‰LLMæ¨ç†"""
    prompt = self._get_prompt(context, output, use_zh)

    # è°ƒç”¨ä½ çš„è‡ªå®šä¹‰æ¨¡å‹
    # ...

    return {
        'prediction': prediction,
        'confidence': confidence,
        'raw_output': generated_text
    }
```

### ä¿®æ”¹Promptæ¨¡æ¿

```python
# åœ¨ LLMInference ç±»ä¸­ä¿®æ”¹è¿™ä¸¤ä¸ªå¸¸é‡

HALLUCINATION_PROMPT_TEMPLATE = """Your custom English prompt..."""

HALLUCINATION_PROMPT_ZH = """ä½ çš„è‡ªå®šä¹‰ä¸­æ–‡æç¤ºè¯..."""
```

### æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

```python
def calculate_custom_metrics(y_true, y_pred):
    """æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡"""
    # å®ç°ä½ çš„è¯„ä¼°é€»è¾‘
    return custom_metrics
```

---

## å‚è€ƒé“¾æ¥

- [Ollamaå®˜ç½‘](https://ollama.ai)
- [Meta Llamaæ–‡æ¡£](https://github.com/facebookresearch/llama)
- [Qwenå®˜æ–¹ä»“åº“](https://github.com/QwenLM/Qwen)
- [HuggingFaceæ¨¡å‹åº“](https://huggingface.co)

---

## åé¦ˆå’Œæ”¹è¿›

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æå‡ºæ”¹è¿›æ„è§ï¼
