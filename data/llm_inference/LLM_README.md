# LLM å¹»è§‰æ£€æµ‹æ¨ç†ç³»ç»Ÿ

ä½¿ç”¨ llama3 å’Œ qwen3 ç­‰å¤§è¯­è¨€æ¨¡å‹å¯¹å¹»è§‰æ£€æµ‹æ•°æ®è¿›è¡Œæ¨ç†ï¼Œæ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒæ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `llm_inference.py` | ğŸ¯ **ä¸»æ¨ç†è„šæœ¬** - æ”¯æŒOllamaã€HuggingFaceã€APIä¸‰ç§éƒ¨ç½²æ–¹å¼ |
| `config_examples.py` | ğŸ“‹ é…ç½®ç¤ºä¾‹ - 8ä¸ªä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®æ¨¡æ¿ |
| `compare_models.py` | ğŸ“Š å¯¹æ¯”åˆ†æå·¥å…· - å¯¹æ¯”BERTå’ŒLLMçš„æ¨ç†ç»“æœ |

### æ–‡æ¡£æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `QUICK_START.md` | âš¡ å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼ˆ5åˆ†é’Ÿä¸Šæ‰‹ï¼‰ |
| `LLM_INFERENCE_GUIDE.md` | ğŸ“– è¯¦ç»†ä½¿ç”¨æŒ‡å—ï¼ˆå®Œæ•´åŠŸèƒ½è¯´æ˜ï¼‰ |
| `requirements_llm.txt` | ğŸ“¦ Pythonä¾èµ–åˆ—è¡¨ |
| `README.md` | ğŸ“„ æœ¬æ–‡ä»¶ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„æ–¹å¼ï¼šä½¿ç”¨Ollamaï¼ˆæ¨èï¼‰

```bash
# 1. å®‰è£…Ollama
curl https://ollama.ai/install.sh | sh

# 2. å¯åŠ¨æœåŠ¡ï¼ˆä¸€ä¸ªç»ˆç«¯ï¼‰
ollama serve

# 3. æ‹‰å–æ¨¡å‹ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
ollama pull llama3
ollama pull qwen:7b

# 4. å®‰è£…ä¾èµ–
pip install -r requirements_llm.txt

# 5. è¿è¡Œæ¨ç†
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --sample_size 50
```

**å®Œæˆï¼** ç»“æœä¿å­˜åœ¨ `./llm_results/` ä¸­

## ğŸ“ å¸¸è§ä½¿ç”¨å‘½ä»¤

### ä½¿ç”¨ä¸åŒæ¨¡å‹

```bash
# llama3ï¼ˆè‹±æ–‡ï¼‰
python llm_inference.py --model_name llama3 --sample_size 100

# qwen 7Bï¼ˆä¸­æ–‡ï¼‰
python llm_inference.py --model_name qwen:7b --use_zh_prompt --sample_size 100

# qwen 14Bï¼ˆä¸­æ–‡ï¼Œæ›´å¥½çš„è´¨é‡ï¼‰
python llm_inference.py --model_name qwen:14b --use_zh_prompt --sample_size 100
```

### å¯¹æ¯”åˆ†æ

```bash
# å¯¹æ¯”å¤šä¸ªæ¨¡å‹
python config_examples.py compare

# å¯¹æ¯”BERTå’ŒLLM
python compare_models.py \
  --bert_dir ./test_results \
  --llm_dir ./llm_results
```

### ä½¿ç”¨å…¨éƒ¨æ•°æ®

```bash
# ä½¿ç”¨æ‰€æœ‰900ä¸ªæµ‹è¯•æ ·æœ¬
python llm_inference.py \
  --model_name llama3 \
  --sample_size None
```

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹

### Ollamaï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰
- **llama3**ï¼šMetaçš„æœ€æ–°æ¨¡å‹ï¼Œæ€§èƒ½å¥½
- **qwen:7b**ï¼šé˜¿é‡Œäº‘QWen 7Bï¼Œé€‚åˆä¸­æ–‡
- **qwen:14b**ï¼šæ›´å¤§çš„QWenï¼Œæ›´å¥½çš„æ€§èƒ½

### HuggingFaceï¼ˆé«˜æ€§èƒ½ï¼‰
- meta-llama/Llama-2-7b-chat-hf
- meta-llama/Llama-2-70b-chat-hf
- THUDM/chatglm-6b
- QwenLM/Qwen-7B-Chat

### APIï¼ˆæ— ç¡¬ä»¶è¦æ±‚ï¼‰
- é˜¿é‡Œäº‘DashScopeï¼ˆQwenç³»åˆ—ï¼‰
- OpenAI / Together AIï¼ˆLlamaç³»åˆ—ï¼‰

## ğŸ“ˆ æ¨ç†ç»“æœ

æ¯æ¬¡æ¨ç†ä¼šç”Ÿæˆä»¥ä¸‹è¾“å‡ºæ–‡ä»¶ï¼š

```
./llm_results/
â”œâ”€â”€ llm_results.json              # è¯„ä¼°æŒ‡æ ‡æ€»ç»“
â”œâ”€â”€ llm_detailed_predictions.xlsx # è¯¦ç»†é¢„æµ‹ç»“æœ
â””â”€â”€ [å¯è§†åŒ–å›¾è¡¨]                   # å¯é€‰çš„å›¾è¡¨
```

### è¾“å‡ºç¤ºä¾‹

```json
{
  "model_name": "llama3",
  "test_size": 900,
  "valid_predictions": 890,
  "detailed_metrics": {
    "accuracy": 0.7651,
    "macro_f1": 0.6178,
    "hallucination": {
      "precision": 0.5145,
      "recall": 0.4234,
      "f1_score": 0.4646
    }
  }
}
```

## ğŸ”§ ä¸‰ç§éƒ¨ç½²æ–¹å¼å¯¹æ¯”

| æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨è |
|------|------|------|------|
| **Ollama** | æœ€ç®€å•ã€è‡ªåŠ¨ä¼˜åŒ–å†…å­˜ | éœ€è¦GPU | âœ… æœ€å¥½é€‰æ‹© |
| **HuggingFace** | å®Œå…¨æ§åˆ¶ã€é«˜æ€§èƒ½ | éœ€è¦ç®¡ç†ä¾èµ– | é«˜çº§ç”¨æˆ· |
| **API** | æ— ç¡¬ä»¶è¦æ±‚ã€æœ€å¼ºæ¨¡å‹ | éœ€è¦APIå¯†é’¥ã€æœ‰è´¹ç”¨ | å¿«é€ŸéªŒè¯ |

## ğŸ“š ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå¿«é€Ÿæµ‹è¯•
```bash
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --sample_size 50 \
  --output_dir ./results/quick_test
```

### ç¤ºä¾‹2ï¼šå®Œæ•´è¯„ä¼°
```bash
python llm_inference.py \
  --model_name qwen:7b \
  --deploy_type ollama \
  --use_zh_prompt \
  --output_dir ./results/qwen_full
```

### ç¤ºä¾‹3ï¼šå¯¹æ¯”å¤šä¸ªæ¨¡å‹
```bash
# ä½¿ç”¨é…ç½®ç¤ºä¾‹è„šæœ¬
python config_examples.py compare

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
python config_examples.py report
```

### ç¤ºä¾‹4ï¼šå¯¹æ¯”BERTå’ŒLLM
```bash
python compare_models.py \
  --bert_dir ./test_results \
  --llm_dir ./llm_results \
  --output_dir ./comparison_results
```

## âš™ï¸ å‘½ä»¤è¡Œå‚æ•°è¯¦è§£

### æ¨¡å‹é…ç½®
- `--model_name`: æ¨¡å‹åç§°ï¼ˆllama3, qwen:7bç­‰ï¼‰
- `--deploy_type`: éƒ¨ç½²æ–¹å¼ï¼ˆollama, huggingface, apiï¼‰
- `--model_path`: HuggingFace/æœ¬åœ°æ¨¡å‹è·¯å¾„

### æ•°æ®é…ç½®
- `--data_path`: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
- `--sample_size`: æŠ½æ ·å¤§å°ï¼ˆNoneä¸ºå…¨éƒ¨ï¼‰

### æ¨ç†é…ç½®
- `--output_dir`: ç»“æœä¿å­˜ç›®å½•
- `--use_zh_prompt`: ä½¿ç”¨ä¸­æ–‡æç¤ºè¯
- `--temperature`: é‡‡æ ·æ¸©åº¦ï¼ˆ0ä¸ºç¡®å®šæ€§ï¼‰
- `--max_tokens`: æœ€å¤§ç”Ÿæˆtokenæ•°

## ğŸ” ç»“æœåˆ†æ

### Pythonåˆ†æç¤ºä¾‹

```python
import json
import pandas as pd

# åŠ è½½ç»“æœ
with open('./llm_results/llm_results.json') as f:
    metrics = json.load(f)

# æ‰“å°å…³é”®æŒ‡æ ‡
print(f"å‡†ç¡®ç‡: {metrics['detailed_metrics']['accuracy']:.2%}")
print(f"å¹»è§‰F1: {metrics['detailed_metrics']['hallucination']['f1_score']:.4f}")

# åŠ è½½è¯¦ç»†é¢„æµ‹
df = pd.read_excel('./llm_results/llm_detailed_predictions.xlsx')

# ç»Ÿè®¡é”™è¯¯åˆ†å¸ƒ
errors = df[df['correct_prediction'] == False]
print(f"é”™è¯¯ç‡: {len(errors)/len(df):.2%}")
```

## ğŸ“– æ›´å¤šæ–‡æ¡£

- **å¿«é€Ÿå¼€å§‹**: `QUICK_START.md` - 5åˆ†é’Ÿä¸Šæ‰‹
- **å®Œæ•´æŒ‡å—**: `LLM_INFERENCE_GUIDE.md` - è¯¦ç»†åŠŸèƒ½è¯´æ˜
- **é…ç½®ç¤ºä¾‹**: `config_examples.py` - 8ä¸ªä½¿ç”¨ç¤ºä¾‹
- **å¯¹æ¯”å·¥å…·**: `compare_models.py` - BERT vs LLMå¯¹æ¯”

## ğŸ› ï¸ æ•…éšœæ’é™¤

### è¿æ¥Ollamaå¤±è´¥
```bash
# ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ
ollama serve

# æ£€æŸ¥å¯ç”¨æ¨¡å‹
ollama list
```

### æ˜¾å­˜ä¸è¶³
- ä½¿ç”¨Ollamaï¼ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰
- é€‰æ‹©æ›´å°çš„æ¨¡å‹ï¼ˆ7B vs 14Bï¼‰
- å‡å°‘æ ·æœ¬æ•°ï¼ˆ--sample_sizeï¼‰

### æ¨ç†é€Ÿåº¦æ…¢
- ä½¿ç”¨GPUï¼ˆæ£€æŸ¥torch.cuda.is_available()ï¼‰
- é€‰æ‹©æ›´å°çš„æ¨¡å‹
- ä½¿ç”¨APIæ–¹å¼

## ğŸ’¡ æç¤º

1. **é¦–æ¬¡è¿è¡Œ**ï¼šä½¿ç”¨--sample_size 50è¿›è¡Œå¿«é€Ÿæµ‹è¯•
2. **å¯¹æ¯”åˆ†æ**ï¼šåˆ†åˆ«è¿è¡ŒBERTå’ŒLLMï¼Œç„¶åç”¨compare_models.pyå¯¹æ¯”
3. **Promptä¼˜åŒ–**ï¼šä¿®æ”¹HALLUCINATION_PROMPT_*æ¥æ”¹è¿›ç»“æœ
4. **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®æ˜¾å­˜é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤§å°

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
1. `QUICK_START.md` ä¸­çš„å¸¸è§é—®é¢˜
2. `LLM_INFERENCE_GUIDE.md` ä¸­çš„è¯¦ç»†è¯´æ˜
3. å‘½ä»¤è¡Œå¸®åŠ©ï¼š`python llm_inference.py --help`

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸé¡¹ç›®çš„è®¸å¯è¯ã€‚

---

**å¼€å§‹ä½¿ç”¨**: [å¿«é€Ÿå¼€å§‹æŒ‡å—](./QUICK_START.md)
