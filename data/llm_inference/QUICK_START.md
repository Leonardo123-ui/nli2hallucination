# ğŸš€ å¿«é€Ÿå¼€å§‹æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©ä½ å¿«é€Ÿå¼€å§‹ä½¿ç”¨LLMè¿›è¡Œå¹»è§‰æ£€æµ‹æ¨ç†ã€‚

## 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### å‰ææ¡ä»¶
- Python 3.8+
- è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆæ¨¡å‹éœ€è¦5-10GBï¼‰

### æ–¹æ³•A: æœ€ç®€å• - ä½¿ç”¨Ollamaï¼ˆæ¨èï¼‰

#### 1. å®‰è£…Ollama
```bash
# è®¿é—® https://ollama.ai ä¸‹è½½å®‰è£…ï¼Œæˆ–ä½¿ç”¨å‘½ä»¤ï¼š
curl https://ollama.ai/install.sh | sh
```

#### 2. å¯åŠ¨OllamaæœåŠ¡
```bash
ollama serve
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç»§ç»­...
```

#### 3. æ‹‰å–æ¨¡å‹ï¼ˆåœ¨æ–°ç»ˆç«¯ä¸­ï¼‰
```bash
# æ‹‰å–llama3ï¼ˆçº¦4GBï¼‰
ollama pull llama3

# æ‹‰å–qwenï¼ˆçº¦3.5GBï¼‰
ollama pull qwen:7b

# éªŒè¯æ¨¡å‹å·²å®‰è£…
ollama list
```

#### 4. å®‰è£…ä¾èµ–
```bash
cd /mnt/nlp/yuanmengying/nli2hallucination/data/bert-classifier
pip install torch transformers pandas numpy scikit-learn tqdm requests
```

#### 5. è¿è¡Œæ¨ç†
```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ50ä¸ªæ ·æœ¬ï¼‰
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --sample_size 50 \
  --output_dir ./results/llama3_test

# å®Œæ•´æµ‹è¯•ï¼ˆ900ä¸ªæ ·æœ¬ï¼‰
python llm_inference.py \
  --model_name llama3 \
  --deploy_type ollama \
  --output_dir ./results/llama3_full
```

**å®Œæˆï¼** ç»“æœä¼šä¿å­˜åœ¨ `./results/llama3_full/` ä¸­

---

### æ–¹æ³•B: å¿«é€Ÿ - ä½¿ç”¨HuggingFaceæ¨¡å‹

#### 1. ä¸‹è½½æ¨¡å‹
```bash
# å¦‚æœæœ‰HuggingFaceè´¦æˆ·ï¼Œè¿™æ ·ä¸‹è½½æœ€å¿«
huggingface-cli login
huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir ./models/llama2

# æˆ–æŒ‡å®šæœ¬åœ°llamaè·¯å¾„
# å¦‚æœä½ å·²ç»æœ‰æœ¬åœ°æ¨¡å‹ï¼Œè·³è¿‡è¿™æ­¥
```

#### 2. è¿è¡Œæ¨ç†
```bash
python llm_inference.py \
  --model_name llama2 \
  --deploy_type huggingface \
  --model_path ./models/llama2 \
  --sample_size 50
```

---

### æ–¹æ³•C: æ— éœ€ä¸‹è½½ - ä½¿ç”¨API

#### é˜¿é‡Œäº‘Qwen API
```bash
# è®¾ç½®APIå¯†é’¥
export DASHSCOPE_API_KEY="your_api_key"
pip install dashscope

# è¿è¡Œæ¨ç†
python llm_inference.py \
  --model_name qwen \
  --deploy_type api \
  --api_key $DASHSCOPE_API_KEY \
  --api_url https://dashscope.aliyuncs.com \
  --sample_size 50
```

---

## å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥è¡¨

```bash
# ä½¿ç”¨llama3
python llm_inference.py --model_name llama3 --sample_size 100

# ä½¿ç”¨qwenï¼ˆä¸­æ–‡ï¼‰
python llm_inference.py --model_name qwen:7b --use_zh_prompt --sample_size 100

# å¯¹æ¯”å¤šä¸ªæ¨¡å‹
python config_examples.py compare

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼ˆBERT vs LLMï¼‰
python compare_models.py

# ä½¿ç”¨å…¨éƒ¨æ•°æ®
python llm_inference.py --model_name llama3 --sample_size None
```

---

## æŸ¥çœ‹ç»“æœ

### å¿«é€ŸæŸ¥çœ‹
```bash
# æŸ¥çœ‹JSONç»“æœæ‘˜è¦
cat ./results/llama3_test/llm_results.json

# ç”¨Excelæ‰“å¼€è¯¦ç»†é¢„æµ‹ç»“æœ
open ./results/llama3_test/llm_detailed_predictions.xlsx
```

### PythonæŸ¥çœ‹
```python
import json
import pandas as pd

# æŸ¥çœ‹è¯„ä¼°æŒ‡æ ‡
with open('./results/llama3_test/llm_results.json') as f:
    results = json.load(f)
    print(results['detailed_metrics'])

# æŸ¥çœ‹è¯¦ç»†é¢„æµ‹
df = pd.read_excel('./results/llama3_test/llm_detailed_predictions.xlsx')
print(df.head())
```

---

## æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | æ˜¾å­˜ | é€Ÿåº¦ | è´¨é‡ | æ¨èç”¨é€” |
|-----|------|------|------|--------|
| llama3 7B | 4GB | å¿« | è‰¯å¥½ | âœ… å¿«é€Ÿæµ‹è¯• |
| qwen:7b | 4GB | å¿« | è‰¯å¥½ | âœ… å¿«é€Ÿæµ‹è¯• |
| llama3 70B | 40GB | æ…¢ | ä¼˜ç§€ | ç²¾åº¦è¦æ±‚é«˜ |
| qwen:14b | 10GB | ä¸­ | ä¼˜ç§€ | å¹³è¡¡é€‰æ‹© |
| API | 0GB | ä¾èµ–ç½‘ç»œ | æœ€ä¼˜ | âœ… æ— ç¡¬ä»¶é™åˆ¶ |

---

## å¸¸è§é—®é¢˜

### Q: æ¨¡å‹ä¸‹è½½å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
**A:** ä½¿ç”¨Ollamaï¼ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰æˆ–APIæ–¹å¼ï¼ˆæ— éœ€ä¸‹è½½ï¼‰

### Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A:** ä½¿ç”¨APIæ–¹å¼æˆ–Ollamaçš„7Bæ¨¡å‹

### Q: æ¨ç†é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
**A:**
- ä½¿ç”¨æ›´å°æ¨¡å‹ï¼ˆ7B vs 14Bï¼‰
- å‡å°‘æ ·æœ¬æ•°ï¼ˆ--sample_sizeï¼‰
- æ£€æŸ¥æ˜¯å¦ç”¨äº†GPU

### Q: ç»“æœå‡†ç¡®ç‡ä½æ€ä¹ˆåŠï¼Ÿ
**A:**
- å°è¯•æ›´å¤§æ¨¡å‹ï¼ˆ14B or 70Bï¼‰
- è°ƒæ•´Promptï¼ˆè§é«˜çº§ç”¨æ³•ï¼‰
- æ•°æ®å¯èƒ½ç¡®å®å›°éš¾

---

## ä¸‹ä¸€æ­¥

- ğŸ“– [è¯¦ç»†ä½¿ç”¨æŒ‡å—](./LLM_INFERENCE_GUIDE.md)
- ğŸ”§ [é…ç½®ç¤ºä¾‹](./config_examples.py)
- ğŸ“Š [å¯¹æ¯”åˆ†æå·¥å…·](./compare_models.py)
- ğŸ’» [å®Œæ•´æºä»£ç ](./llm_inference.py)

---

## éœ€è¦å¸®åŠ©ï¼Ÿ

æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: `LLM_INFERENCE_GUIDE.md`

æŸ¥çœ‹ç¤ºä¾‹ä»£ç : `config_examples.py`

```bash
# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
python llm_inference.py --help
python compare_models.py --help
```

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ğŸ‰**
