#!/usr/bin/env python3
"""
LLMæ¨ç†ç¯å¢ƒæ£€æŸ¥è„šæœ¬
æ£€æŸ¥æ˜¯å¦å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–å’ŒæœåŠ¡
"""

import sys
import subprocess
from pathlib import Path


def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print("âœ“ Pythonç‰ˆæœ¬æ£€æŸ¥")
    version = sys.version_info
    if version.major >= 3 and version.minor >= 8:
        print(f"  âœ… Python {version.major}.{version.minor}.{version.micro}")
        return True
    else:
        print(f"  âŒ Pythonç‰ˆæœ¬è¿‡ä½: {version.major}.{version.minor}")
        print("     éœ€è¦ Python 3.8+")
        return False


def check_package(package_name, import_name=None):
    """æ£€æŸ¥PythonåŒ…"""
    if import_name is None:
        import_name = package_name.replace('-', '_')

    try:
        __import__(import_name)
        print(f"  âœ… {package_name}")
        return True
    except ImportError:
        print(f"  âŒ {package_name} (æœªå®‰è£…)")
        return False


def check_python_packages():
    """æ£€æŸ¥å¿…è¦çš„PythonåŒ…"""
    print("\nâœ“ PythonåŒ…æ£€æŸ¥")

    packages = [
        ("torch", "torch"),
        ("transformers", "transformers"),
        ("pandas", "pandas"),
        ("numpy", "numpy"),
        ("scikit-learn", "sklearn"),
        ("tqdm", "tqdm"),
        ("requests", "requests"),
    ]

    results = []
    for package, import_name in packages:
        results.append(check_package(package, import_name))

    return all(results)


def check_torch_cuda():
    """æ£€æŸ¥CUDAæ”¯æŒ"""
    print("\nâœ“ CUDA/GPUæ£€æŸ¥")
    try:
        import torch
        if torch.cuda.is_available():
            print(f"  âœ… CUDAå¯ç”¨ (GPU: {torch.cuda.get_device_name(0)})")
            print(f"     CUDAç‰ˆæœ¬: {torch.version.cuda}")
            return True
        else:
            print("  âš ï¸  CUDAä¸å¯ç”¨ï¼ˆä½¿ç”¨CPUä¼šå¾ˆæ…¢ï¼‰")
            return False
    except Exception as e:
        print(f"  âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_ollama():
    """æ£€æŸ¥OllamaæœåŠ¡"""
    print("\nâœ“ OllamaæœåŠ¡æ£€æŸ¥")
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            models = response.json().get('models', [])
            if models:
                print(f"  âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")
                print(f"     å¯ç”¨æ¨¡å‹: {len(models)}")
                for model in models[:3]:
                    name = model.get('name', 'unknown')
                    print(f"     - {name}")
                return True
            else:
                print("  âš ï¸  Ollamaè¿è¡Œä¸­ï¼Œä½†æ²¡æœ‰æ¨¡å‹")
                print("     è¿è¡Œ: ollama pull llama3")
                return False
        else:
            print(f"  âš ï¸  Ollamaå“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("  âŒ OllamaæœåŠ¡æœªè¿è¡Œ")
        print("     å¯åŠ¨: ollama serve")
        return False
    except Exception as e:
        print(f"  âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_model_files():
    """æ£€æŸ¥æ‰€éœ€çš„è„šæœ¬æ–‡ä»¶"""
    print("\nâœ“ è„šæœ¬æ–‡ä»¶æ£€æŸ¥")

    required_files = [
        "llm_inference.py",
        "config_examples.py",
        "compare_models.py",
    ]

    doc_files = [
        "QUICK_START.md",
        "LLM_INFERENCE_GUIDE.md",
        "LLM_README.md",
    ]

    all_exist = True
    for file in required_files:
        path = Path(file)
        if path.exists():
            print(f"  âœ… {file}")
        else:
            print(f"  âŒ {file} (æœªæ‰¾åˆ°)")
            all_exist = False

    print("\n  æ–‡æ¡£æ–‡ä»¶:")
    for file in doc_files:
        path = Path(file)
        if path.exists():
            print(f"  âœ… {file}")
        else:
            print(f"  âš ï¸  {file} (å¯é€‰)")

    return all_exist


def check_data_files():
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    print("\nâœ“ æ•°æ®æ–‡ä»¶æ£€æŸ¥")

    data_path = Path("../summary_nli_hallucination_dataset.xlsx")
    if data_path.exists():
        size = data_path.stat().st_size / (1024 * 1024)
        print(f"  âœ… æµ‹è¯•æ•°æ®å­˜åœ¨ ({size:.2f}MB)")
        return True
    else:
        print(f"  âŒ æµ‹è¯•æ•°æ®æœªæ‰¾åˆ°: {data_path}")
        return False


def print_summary(results):
    """æ‰“å°æ£€æŸ¥æ€»ç»“"""
    print("\n" + "="*60)
    print("ç¯å¢ƒæ£€æŸ¥æ€»ç»“")
    print("="*60)

    all_pass = all(results.values())

    status = "âœ… ç¯å¢ƒå°±ç»ªï¼" if all_pass else "âš ï¸  å­˜åœ¨æœªæ»¡è¶³çš„è¦æ±‚"
    print(f"\n{status}\n")

    if not all_pass:
        if not results.get('python_version'):
            print("éœ€è¦å‡çº§Pythonåˆ°3.8+")

        if not results.get('packages'):
            print("\nå®‰è£…ä¾èµ–åŒ…:")
            print("  pip install -r requirements_llm.txt")

        if not results.get('ollama'):
            print("\nè¦ä½¿ç”¨Ollamaï¼Œéœ€è¦:")
            print("  1. å®‰è£…: curl https://ollama.ai/install.sh | sh")
            print("  2. å¯åŠ¨: ollama serve")
            print("  3. æ‹‰å–æ¨¡å‹: ollama pull llama3")

        if not results.get('files'):
            print("\nè„šæœ¬æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ç›®å½•")

        if not results.get('data'):
            print("\næ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•")


def print_quick_start():
    """æ‰“å°å¿«é€Ÿå¼€å§‹æç¤º"""
    print("\n" + "="*60)
    print("å¿«é€Ÿå¼€å§‹")
    print("="*60)

    print("""
1ï¸âƒ£  å¿«é€Ÿæµ‹è¯•ï¼ˆ50ä¸ªæ ·æœ¬ï¼‰:
  python llm_inference.py --model_name llama3 --sample_size 50

2ï¸âƒ£  å®Œæ•´æµ‹è¯•ï¼š
  python llm_inference.py --model_name llama3

3ï¸âƒ£  ä½¿ç”¨ä¸­æ–‡æç¤ºè¯ï¼š
  python llm_inference.py --model_name qwen:7b --use_zh_prompt

4ï¸âƒ£  å¯¹æ¯”å¤šä¸ªæ¨¡å‹ï¼š
  python config_examples.py compare

5ï¸âƒ£  æŸ¥çœ‹å¸®åŠ©ï¼š
  python llm_inference.py --help

ğŸ“– æ›´å¤šä¿¡æ¯: æŸ¥çœ‹ QUICK_START.md æˆ– LLM_INFERENCE_GUIDE.md
""")


def main():
    """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
    print("="*60)
    print("LLMæ¨ç†ç¯å¢ƒæ£€æŸ¥")
    print("="*60 + "\n")

    results = {
        'python_version': check_python_version(),
        'packages': check_python_packages(),
        'cuda': check_torch_cuda(),
        'ollama': check_ollama(),
        'files': check_model_files(),
        'data': check_data_files(),
    }

    print_summary(results)

    if all(results.values()):
        print_quick_start()


if __name__ == "__main__":
    main()
