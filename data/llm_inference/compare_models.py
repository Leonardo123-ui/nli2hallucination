"""
å¯¹æ¯”BERTåˆ†ç±»å™¨å’ŒLLMæ¨ç†ç»“æœ
"""

import pandas as pd
import json
import numpy as np
from pathlib import Path
from typing import Tuple
import matplotlib.pyplot as plt
import seaborn as sns


class ModelComparison:
    """æ¨¡å‹å¯¹æ¯”ç±»"""

    def __init__(self, bert_results_dir="./test_results", llm_results_dir="./llm_results"):
        """åˆå§‹åŒ–å¯¹æ¯”å·¥å…·"""
        self.bert_dir = Path(bert_results_dir)
        self.llm_dir = Path(llm_results_dir)

    def load_bert_results(self) -> Tuple[pd.DataFrame, dict]:
        """åŠ è½½BERTç»“æœ"""
        print("åŠ è½½BERTç»“æœ...")

        # åŠ è½½è¯¦ç»†é¢„æµ‹ç»“æœ
        predictions_file = self.bert_dir / "detailed_predictions.xlsx"
        if not predictions_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {predictions_file}")

        bert_df = pd.read_excel(predictions_file)

        # åŠ è½½è¯„ä¼°æŒ‡æ ‡
        metrics_file = self.bert_dir / "test_results.json"
        if metrics_file.exists():
            with open(metrics_file, 'r', encoding='utf-8') as f:
                bert_metrics = json.load(f)
        else:
            bert_metrics = None

        return bert_df, bert_metrics

    def load_llm_results(self, model_name: str = "llama3") -> Tuple[pd.DataFrame, dict]:
        """åŠ è½½LLMç»“æœ"""
        print(f"åŠ è½½LLM({model_name})ç»“æœ...")

        llm_model_dir = self.llm_dir / model_name
        if not llm_model_dir.exists():
            # å°è¯•å…¶ä»–å¯èƒ½çš„ç›®å½•å
            possible_dirs = list(self.llm_dir.glob("*"))
            if possible_dirs:
                llm_model_dir = possible_dirs[0]
                print(f"ä½¿ç”¨ç›®å½•: {llm_model_dir}")
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°LLMç»“æœç›®å½•: {self.llm_dir}")

        # åŠ è½½è¯¦ç»†é¢„æµ‹ç»“æœ
        predictions_file = llm_model_dir / "llm_detailed_predictions.xlsx"
        if not predictions_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {predictions_file}")

        llm_df = pd.read_excel(predictions_file)

        # åŠ è½½è¯„ä¼°æŒ‡æ ‡
        metrics_file = llm_model_dir / "llm_results.json"
        if metrics_file.exists():
            with open(metrics_file, 'r', encoding='utf-8') as f:
                llm_metrics = json.load(f)
        else:
            llm_metrics = None

        return llm_df, llm_metrics

    def merge_results(self, bert_df: pd.DataFrame, llm_df: pd.DataFrame) -> pd.DataFrame:
        """åˆå¹¶BERTå’ŒLLMçš„ç»“æœ"""
        print("åˆå¹¶ç»“æœ...")

        # å°†llmé¢„æµ‹ç»“æœæ·»åŠ åˆ°bertç»“æœ
        bert_df['llm_prediction'] = None
        bert_df['llm_confidence'] = None

        # ä½¿ç”¨idå­—æ®µè¿›è¡Œåˆå¹¶
        if 'id' in bert_df.columns and 'id' in llm_df.columns:
            llm_map = dict(zip(llm_df['id'], llm_df['llm_prediction']))
            conf_map = dict(zip(llm_df['id'], llm_df['llm_confidence']))

            for idx, row in bert_df.iterrows():
                sample_id = row['id']
                if sample_id in llm_map:
                    bert_df.loc[idx, 'llm_prediction'] = llm_map[sample_id]
                    bert_df.loc[idx, 'llm_confidence'] = conf_map[sample_id]
        else:
            # å¦‚æœæ²¡æœ‰idå­—æ®µï¼ŒæŒ‰é¡ºåºå¯¹é½
            bert_df['llm_prediction'] = llm_df['llm_prediction'].values[:len(bert_df)]
            bert_df['llm_confidence'] = llm_df['llm_confidence'].values[:len(bert_df)]

        # æ·»åŠ ä¸€è‡´æ€§æ ‡è®°
        bert_df['bert_correct'] = bert_df['predicted_label'] == bert_df['label']
        bert_df['llm_correct'] = bert_df['llm_prediction'] == bert_df['label']
        bert_df['both_correct'] = bert_df['bert_correct'] & bert_df['llm_correct']
        bert_df['both_wrong'] = (~bert_df['bert_correct']) & (~bert_df['llm_correct'])
        bert_df['disagreement'] = bert_df['predicted_label'] != bert_df['llm_prediction']

        return bert_df

    def calculate_comparison_metrics(self, comparison_df: pd.DataFrame) -> dict:
        """è®¡ç®—å¯¹æ¯”æŒ‡æ ‡"""
        print("è®¡ç®—å¯¹æ¯”æŒ‡æ ‡...")

        total = len(comparison_df)

        # ä¸€è‡´æ€§æŒ‡æ ‡
        agreement = (comparison_df['predicted_label'] == comparison_df['llm_prediction']).sum() / total
        both_correct = comparison_df['both_correct'].sum() / total
        both_wrong = comparison_df['both_wrong'].sum() / total
        bert_only_correct = (comparison_df['bert_correct'] & ~comparison_df['llm_correct']).sum() / total
        llm_only_correct = (~comparison_df['bert_correct'] & comparison_df['llm_correct']).sum() / total

        # é’ˆå¯¹æœ‰å¹»è§‰ç±»åˆ«çš„åˆ†æ
        hallucination_mask = comparison_df['label'] == 1
        hallucination_df = comparison_df[hallucination_mask]

        if len(hallucination_df) > 0:
            hall_agreement = (hallucination_df['predicted_label'] == hallucination_df['llm_prediction']).sum() / len(
                hallucination_df)
            hall_bert_recall = hallucination_df['bert_correct'].sum() / len(hallucination_df)
            hall_llm_recall = hallucination_df['llm_correct'].sum() / len(hallucination_df)
        else:
            hall_agreement = 0
            hall_bert_recall = 0
            hall_llm_recall = 0

        # é’ˆå¯¹æ— å¹»è§‰ç±»åˆ«çš„åˆ†æ
        no_hallucination_mask = comparison_df['label'] == 0
        no_hallucination_df = comparison_df[no_hallucination_mask]

        if len(no_hallucination_df) > 0:
            no_hall_agreement = (no_hallucination_df['predicted_label'] == no_hallucination_df['llm_prediction']).sum() / len(
                no_hallucination_df)
            no_hall_bert_recall = no_hallucination_df['bert_correct'].sum() / len(no_hallucination_df)
            no_hall_llm_recall = no_hallucination_df['llm_correct'].sum() / len(no_hallucination_df)
        else:
            no_hall_agreement = 0
            no_hall_bert_recall = 0
            no_hall_llm_recall = 0

        return {
            'total_samples': total,
            'agreement_rate': agreement,
            'both_correct_rate': both_correct,
            'both_wrong_rate': both_wrong,
            'bert_only_correct_rate': bert_only_correct,
            'llm_only_correct_rate': llm_only_correct,
            'hallucination_agreement': hall_agreement,
            'hallucination_bert_recall': hall_bert_recall,
            'hallucination_llm_recall': hall_llm_recall,
            'no_hallucination_agreement': no_hall_agreement,
            'no_hallucination_bert_recall': no_hall_bert_recall,
            'no_hallucination_llm_recall': no_hall_llm_recall,
        }

    def print_comparison_report(
        self,
        comparison_df: pd.DataFrame,
        bert_metrics: dict,
        llm_metrics: dict,
        comparison_metrics: dict
    ):
        """æ‰“å°å¯¹æ¯”æŠ¥å‘Š"""

        print("\n" + "="*80)
        print("BERT vs LLM å¹»è§‰æ£€æµ‹å¯¹æ¯”æŠ¥å‘Š")
        print("="*80)

        print("\nğŸ“Š æ€»ä½“ä¸€è‡´æ€§åˆ†æ:")
        print(f"æ ·æœ¬æ€»æ•°: {comparison_metrics['total_samples']}")
        print(f"é¢„æµ‹ä¸€è‡´ç‡: {comparison_metrics['agreement_rate']:.2%}")
        print(f"ä¸¤ä¸ªæ¨¡å‹éƒ½æ­£ç¡®: {comparison_metrics['both_correct_rate']:.2%}")
        print(f"ä¸¤ä¸ªæ¨¡å‹éƒ½é”™è¯¯: {comparison_metrics['both_wrong_rate']:.2%}")
        print(f"ä»…BERTæ­£ç¡®: {comparison_metrics['bert_only_correct_rate']:.2%}")
        print(f"ä»…LLMæ­£ç¡®: {comparison_metrics['llm_only_correct_rate']:.2%}")

        print("\nğŸ” æœ‰å¹»è§‰æ ·æœ¬ (æ ‡ç­¾=1) åˆ†æ:")
        print(f"é¢„æµ‹ä¸€è‡´ç‡: {comparison_metrics['hallucination_agreement']:.2%}")
        print(f"BERTå¬å›ç‡: {comparison_metrics['hallucination_bert_recall']:.2%}")
        print(f"LLMå¬å›ç‡: {comparison_metrics['hallucination_llm_recall']:.2%}")

        print("\nâœ… æ— å¹»è§‰æ ·æœ¬ (æ ‡ç­¾=0) åˆ†æ:")
        print(f"é¢„æµ‹ä¸€è‡´ç‡: {comparison_metrics['no_hallucination_agreement']:.2%}")
        print(f"BERTå‡†ç¡®ç‡: {comparison_metrics['no_hallucination_bert_recall']:.2%}")
        print(f"LLMå‡†ç¡®ç‡: {comparison_metrics['no_hallucination_llm_recall']:.2%}")

        print("\nğŸ“ˆ BERTæ€§èƒ½æŒ‡æ ‡:")
        if bert_metrics:
            metrics = bert_metrics['detailed_metrics']
            print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
            print(f"å¹»è§‰F1åˆ†æ•°: {metrics['hallucination']['f1_score']:.4f}")
            print(f"å¹»è§‰å¬å›ç‡: {metrics['hallucination']['recall']:.4f}")

        print("\nğŸ“ˆ LLMæ€§èƒ½æŒ‡æ ‡:")
        if llm_metrics:
            metrics = llm_metrics['detailed_metrics']
            print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
            print(f"å¹»è§‰F1åˆ†æ•°: {metrics['hallucination']['f1_score']:.4f}")
            print(f"å¹»è§‰å¬å›ç‡: {metrics['hallucination']['recall']:.4f}")

        print("\n" + "="*80)

    def save_comparison_results(self, comparison_df: pd.DataFrame, output_dir="./comparison_results"):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        # ä¿å­˜å®Œæ•´å¯¹æ¯”ç»“æœ
        output_file = Path(output_dir) / "bert_llm_comparison.xlsx"
        comparison_df.to_excel(output_file, index=False)
        print(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_file}")

        # ä¿å­˜é”™è¯¯åˆ†æ
        disagreement_df = comparison_df[comparison_df['disagreement']]
        disagreement_file = Path(output_dir) / "disagreement_cases.xlsx"
        disagreement_df.to_excel(disagreement_file, index=False)
        print(f"åˆ†æ­§æ¡ˆä¾‹å·²ä¿å­˜: {disagreement_file}")

        # ä¿å­˜ä¸¤ä¸ªæ¨¡å‹éƒ½é”™è¯¯çš„æƒ…å†µ
        both_wrong_df = comparison_df[comparison_df['both_wrong']]
        both_wrong_file = Path(output_dir) / "both_wrong_cases.xlsx"
        both_wrong_df.to_excel(both_wrong_file, index=False)
        print(f"ä¸¤ä¸ªæ¨¡å‹éƒ½é”™è¯¯çš„æƒ…å†µå·²ä¿å­˜: {both_wrong_file}")

        # ä¿å­˜ä¸€è‡´æ€§ç»Ÿè®¡
        consistency_stats = {
            'åˆ†ç±»': ['é¢„æµ‹ä¸€è‡´', 'ä¸¤ä¸ªéƒ½æ­£ç¡®', 'ä¸¤ä¸ªéƒ½é”™è¯¯', 'ä»…BERTæ­£ç¡®', 'ä»…LLMæ­£ç¡®'],
            'æ•°é‡': [
                (comparison_df['predicted_label'] == comparison_df['llm_prediction']).sum(),
                comparison_df['both_correct'].sum(),
                comparison_df['both_wrong'].sum(),
                (comparison_df['bert_correct'] & ~comparison_df['llm_correct']).sum(),
                (~comparison_df['bert_correct'] & comparison_df['llm_correct']).sum(),
            ]
        }
        stats_df = pd.DataFrame(consistency_stats)
        stats_file = Path(output_dir) / "consistency_statistics.xlsx"
        stats_df.to_excel(stats_file, index=False)
        print(f"ä¸€è‡´æ€§ç»Ÿè®¡å·²ä¿å­˜: {stats_file}")

    def run_full_comparison(self, output_dir="./comparison_results"):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”åˆ†æ"""
        # åŠ è½½ç»“æœ
        bert_df, bert_metrics = self.load_bert_results()
        llm_df, llm_metrics = self.load_llm_results()

        # åˆå¹¶ç»“æœ
        comparison_df = self.merge_results(bert_df, llm_df)

        # è®¡ç®—å¯¹æ¯”æŒ‡æ ‡
        comparison_metrics = self.calculate_comparison_metrics(comparison_df)

        # æ‰“å°æŠ¥å‘Š
        self.print_comparison_report(comparison_df, bert_metrics, llm_metrics, comparison_metrics)

        # ä¿å­˜ç»“æœ
        self.save_comparison_results(comparison_df, output_dir)

        return comparison_df, comparison_metrics


def main():
    import argparse

    parser = argparse.ArgumentParser(description='å¯¹æ¯”BERTå’ŒLLMçš„æ¨ç†ç»“æœ')
    parser.add_argument('--bert_dir', default='./test_results',
                       help='BERTç»“æœç›®å½•')
    parser.add_argument('--llm_dir', default='./llm_results',
                       help='LLMç»“æœç›®å½•')
    parser.add_argument('--output_dir', default='./comparison_results',
                       help='å¯¹æ¯”ç»“æœä¿å­˜ç›®å½•')

    args = parser.parse_args()

    try:
        # åˆ›å»ºå¯¹æ¯”å·¥å…·
        comparator = ModelComparison(
            bert_results_dir=args.bert_dir,
            llm_results_dir=args.llm_dir
        )

        # è¿è¡Œå¯¹æ¯”åˆ†æ
        comparison_df, metrics = comparator.run_full_comparison(args.output_dir)

        print(f"\nâœ… å¯¹æ¯”åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")

    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²è¿è¡ŒBERTå’ŒLLMçš„æ¨ç†è„šæœ¬")


if __name__ == "__main__":
    main()
