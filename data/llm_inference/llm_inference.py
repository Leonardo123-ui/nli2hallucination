"""
LLMæ¨ç†è„šæœ¬ï¼šä½¿ç”¨llama3å’Œqwen3è¿›è¡Œå¹»è§‰æ£€æµ‹
æ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ï¼šOllamaæœåŠ¡ã€HuggingFaceç›´æ¥åŠ è½½ã€APIè°ƒç”¨
"""

import os
import torch
import pandas as pd
import numpy as np
import json
import argparse
import logging
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import requests
from dataclasses import dataclass
from enum import Enum
from tqdm import tqdm

from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, confusion_matrix,
    roc_auc_score, classification_report
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelDeployType(Enum):
    """æ¨¡å‹éƒ¨ç½²ç±»å‹"""
    OLLAMA = "ollama"  # æœ¬åœ°OllamaæœåŠ¡
    HUGGINGFACE = "huggingface"  # HuggingFaceç›´æ¥åŠ è½½
    API = "api"  # APIæ–¹å¼ï¼ˆé˜¿é‡Œäº‘ã€OpenAIç­‰ï¼‰


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    model_name: str
    deploy_type: ModelDeployType
    model_path: Optional[str] = None  # HuggingFaceæ¨¡å‹è·¯å¾„æˆ–æœ¬åœ°è·¯å¾„
    ollama_url: str = "http://localhost:11434"  # OllamaæœåŠ¡åœ°å€
    api_key: Optional[str] = None
    api_url: Optional[str] = None
    temperature: float = 0.0
    max_tokens: int = 100
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


class LLMInference:
    """LLMæ¨ç†ç±»"""

    # å¹»è§‰æ£€æµ‹çš„promptæ¨¡æ¿
    HALLUCINATION_PROMPT_TEMPLATE = """You are an expert in detecting hallucinations in generated text.

Given the context and the generated output, determine if the output contains hallucinations (statements that are not supported by or contradicted by the context).

Context: {context}

Generated Output: {output}

Task: Determine if the output contains hallucinations.
- Output "hallucination" if the output contains hallucinations (statements not supported by context)
- Output "no_hallucination" if the output is factually consistent with the context

Answer (only output 'hallucination' or 'no_hallucination'): """

    # ä¸­æ–‡ç‰ˆæœ¬çš„prompt
    HALLUCINATION_PROMPT_ZH = """ä½ æ˜¯ä¸€ä½ä¸“é—¨æ£€æµ‹ç”Ÿæˆæ–‡æœ¬ä¸­å¹»è§‰çš„ä¸“å®¶ã€‚

å¹»è§‰æ˜¯æŒ‡åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­å‡ºç°çš„ã€åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ä¾æ®æˆ–ä¸ä¸Šä¸‹æ–‡çŸ›ç›¾çš„è¯­å¥ã€‚

ä¸Šä¸‹æ–‡ï¼š{context}

ç”Ÿæˆçš„è¾“å‡ºï¼š{output}

ä»»åŠ¡ï¼šåˆ¤æ–­è¾“å‡ºæ˜¯å¦åŒ…å«å¹»è§‰ã€‚
- å¦‚æœè¾“å‡ºä¸­æœ‰ä¸ç¬¦åˆä¸Šä¸‹æ–‡çš„è¯­å¥ï¼Œè¾“å‡º"hallucination"
- å¦‚æœè¾“å‡ºåœ¨äº‹å®ä¸Šä¸ä¸Šä¸‹æ–‡ä¸€è‡´ï¼Œè¾“å‡º"no_hallucination"

ç­”æ¡ˆï¼ˆåªè¾“å‡º'hallucination'æˆ–'no_hallucination'ï¼‰ï¼š"""

    def __init__(self, config: LLMConfig):
        """åˆå§‹åŒ–LLMæ¨ç†å™¨"""
        self.config = config
        self.model = None
        self.tokenizer = None

        if config.deploy_type == ModelDeployType.HUGGINGFACE:
            self._load_huggingface_model()
        elif config.deploy_type == ModelDeployType.OLLAMA:
            self._init_ollama()
        elif config.deploy_type == ModelDeployType.API:
            self._init_api()

    def _load_huggingface_model(self):
        """åŠ è½½HuggingFaceæ¨¡å‹"""
        logger.info(f"æ­£åœ¨åŠ è½½HuggingFaceæ¨¡å‹: {self.config.model_path}")
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM

            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)

            # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨quantizationå‡å°‘å†…å­˜å ç”¨
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                # load_in_8bit=True  # 8-bit quantization
            )
            self.model.eval()

            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _init_ollama(self):
        """åˆå§‹åŒ–Ollamaè¿æ¥"""
        logger.info(f"è¿æ¥OllamaæœåŠ¡: {self.config.ollama_url}")
        # æµ‹è¯•è¿æ¥
        try:
            response = requests.get(f"{self.config.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'].split(':')[0] for m in models]
                logger.info(f"Ollamaå¯ç”¨æ¨¡å‹: {model_names}")
            else:
                logger.warning(f"OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
        except requests.exceptions.ConnectionError:
            logger.error(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ {self.config.ollama_url}")
            logger.info("è¯·ç¡®ä¿Ollamaå·²å¯åŠ¨: ollama serve")
            raise

    def _init_api(self):
        """åˆå§‹åŒ–APIè¿æ¥"""
        logger.info(f"ä½¿ç”¨API: {self.config.api_url}")
        if not self.config.api_key:
            logger.warning("æœªæä¾›APIå¯†é’¥")

    def _get_prompt(self, context: str, output: str, use_zh: bool = False) -> str:
        """ç”Ÿæˆæç¤ºè¯"""
        if use_zh:
            return self.HALLUCINATION_PROMPT_ZH.format(context=context, output=output)
        else:
            return self.HALLUCINATION_PROMPT_TEMPLATE.format(context=context, output=output)

    def predict_ollama(self, context: str, output: str, use_zh: bool = False) -> Dict:
        """ä½¿ç”¨Ollamaè¿›è¡Œæ¨ç†"""
        prompt = self._get_prompt(context, output, use_zh)

        try:
            response = requests.post(
                f"{self.config.ollama_url}/api/generate",
                json={
                    "model": self.config.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "temperature": self.config.temperature,
                },
                timeout=120
            )

            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '').strip().lower()

                # è§£æç»“æœ
                if 'hallucination' in generated_text:
                    prediction = 1  # æœ‰å¹»è§‰
                    confidence = 0.8
                elif 'no_hallucination' in generated_text:
                    prediction = 0  # æ— å¹»è§‰
                    confidence = 0.8
                else:
                    # å¦‚æœæ¨¡å‹æ²¡æœ‰æ˜ç¡®è¾“å‡ºï¼Œå°è¯•å…¶ä»–å¯å‘å¼æ–¹æ³•
                    prediction = 1 if 'hallucination' in generated_text else 0
                    confidence = 0.5

                return {
                    'prediction': prediction,
                    'confidence': confidence,
                    'raw_output': generated_text
                }
            else:
                logger.error(f"Ollama APIé”™è¯¯: {response.status_code}")
                return {'prediction': -1, 'confidence': 0, 'raw_output': ''}

        except Exception as e:
            logger.error(f"Ollamaæ¨ç†é”™è¯¯: {e}")
            return {'prediction': -1, 'confidence': 0, 'raw_output': str(e)}

    def predict_huggingface(self, context: str, output: str, use_zh: bool = False) -> Dict:
        """ä½¿ç”¨HuggingFaceæ¨¡å‹è¿›è¡Œæ¨ç†"""
        prompt = self._get_prompt(context, output, use_zh)

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=2048, truncation=True)
            inputs = {k: v.to(self.config.device) for k, v in inputs.items()}
            eos_id = self.model.config.eos_token_id
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.max_tokens,
                    temperature=self.config.temperature,
                    pad_token_id=eos_id,
                    do_sample=False
                )

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # æå–promptä¹‹åçš„éƒ¨åˆ†
            generated_text = generated_text[len(prompt):].strip().lower()

            # è§£æç»“æœ
            if 'hallucination' in generated_text:
                prediction = 1
                confidence = 0.8
            elif 'no_hallucination' in generated_text:
                prediction = 0
                confidence = 0.8
            else:
                prediction = 1 if 'hallucination' in generated_text else 0
                confidence = 0.5

            return {
                'prediction': prediction,
                'confidence': confidence,
                'raw_output': generated_text
            }

        except Exception as e:
            logger.error(f"HuggingFaceæ¨ç†é”™è¯¯: {e}")
            return {'prediction': -1, 'confidence': 0, 'raw_output': str(e)}

    def predict_api(self, context: str, output: str, use_zh: bool = False) -> Dict:
        """ä½¿ç”¨APIè¿›è¡Œæ¨ç†ï¼ˆç¤ºä¾‹ï¼šé˜¿é‡Œäº‘Qwenï¼‰"""
        prompt = self._get_prompt(context, output, use_zh)

        try:
            # ç¤ºä¾‹ï¼šé˜¿é‡Œäº‘Qwen API
            if 'aliyun' in self.config.api_url.lower() or 'dashscope' in self.config.api_url.lower():
                return self._predict_aliyun_qwen(prompt)
            # ç¤ºä¾‹ï¼šOpenAI APIï¼ˆæ”¯æŒllamaï¼‰
            elif 'openai' in self.config.api_url.lower():
                return self._predict_openai(prompt)
            else:
                logger.error(f"ä¸æ”¯æŒçš„API: {self.config.api_url}")
                return {'prediction': -1, 'confidence': 0, 'raw_output': ''}

        except Exception as e:
            logger.error(f"APIæ¨ç†é”™è¯¯: {e}")
            return {'prediction': -1, 'confidence': 0, 'raw_output': str(e)}

    def _predict_aliyun_qwen(self, prompt: str) -> Dict:
        """è°ƒç”¨é˜¿é‡Œäº‘Qwen API"""
        try:
            from dashscope import Generation

            response = Generation.call(
                model="qwen-max",  # æˆ– qwen-plus, qwen-turboç­‰
                messages=[{'role': 'user', 'content': prompt}],
                api_key=self.config.api_key,
                temperature=self.config.temperature,
            )

            if response.status_code == 200:
                generated_text = response.output.text.lower()

                if 'hallucination' in generated_text:
                    prediction = 1
                    confidence = 0.8
                elif 'no_hallucination' in generated_text:
                    prediction = 0
                    confidence = 0.8
                else:
                    prediction = 1 if 'hallucination' in generated_text else 0
                    confidence = 0.5

                return {
                    'prediction': prediction,
                    'confidence': confidence,
                    'raw_output': generated_text
                }
            else:
                logger.error(f"é˜¿é‡Œäº‘APIé”™è¯¯: {response.message}")
                return {'prediction': -1, 'confidence': 0, 'raw_output': ''}

        except ImportError:
            logger.error("è¯·å®‰è£…dashscopeåº“: pip install dashscope")
            raise

    def _predict_openai(self, prompt: str) -> Dict:
        """è°ƒç”¨OpenAIå…¼å®¹çš„API"""
        try:
            import openai

            openai.api_key = self.config.api_key
            openai.api_base = self.config.api_url

            response = openai.ChatCompletion.create(
                model=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )

            generated_text = response.choices[0].message.content.lower()

            if 'hallucination' in generated_text:
                prediction = 1
                confidence = 0.8
            elif 'no_hallucination' in generated_text:
                prediction = 0
                confidence = 0.8
            else:
                prediction = 1 if 'hallucination' in generated_text else 0
                confidence = 0.5

            return {
                'prediction': prediction,
                'confidence': confidence,
                'raw_output': generated_text
            }

        except ImportError:
            logger.error("è¯·å®‰è£…openaiåº“: pip install openai")
            raise

    def predict(self, context: str, output: str, use_zh: bool = False) -> Dict:
        """æ ¹æ®é…ç½®çš„éƒ¨ç½²æ–¹å¼è¿›è¡Œæ¨ç†"""
        if self.config.deploy_type == ModelDeployType.OLLAMA:
            return self.predict_ollama(context, output, use_zh)
        elif self.config.deploy_type == ModelDeployType.HUGGINGFACE:
            return self.predict_huggingface(context, output, use_zh)
        elif self.config.deploy_type == ModelDeployType.API:
            return self.predict_api(context, output, use_zh)
        else:
            raise ValueError(f"æœªçŸ¥çš„éƒ¨ç½²ç±»å‹: {self.config.deploy_type}")


def load_test_data(data_path: str) -> Tuple[pd.DataFrame, List[str]]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    logger.info(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {data_path}")

    full_df = pd.read_excel(data_path, sheet_name='NLIæ•°æ®é›†')
    test_df = full_df[full_df['split'] == 'test'].reset_index(drop=True)

    logger.info(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}")

    return test_df


def calculate_detailed_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)
    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')

    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0

    return {
        'accuracy': accuracy,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'no_hallucination': {
            'precision': precision[0],
            'recall': recall[0],
            'f1_score': f1[0],
            'support': int(support[0])
        },
        'hallucination': {
            'precision': precision[1],
            'recall': recall[1],
            'f1_score': f1[1],
            'support': int(support[1])
        },
        'confusion_matrix': {
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp)
        },
        'specificity': specificity,
        'sensitivity': sensitivity,
        'false_positive_rate': fpr,
        'false_negative_rate': fnr,
    }


def run_inference(
    model_config: LLMConfig,
    data_path: str,
    output_dir: str = './llm_results',
    use_zh_prompt: bool = False,
    sample_size: Optional[int] = None
):
    """è¿è¡Œæ¨ç†"""

    os.makedirs(output_dir, exist_ok=True)

    # åˆå§‹åŒ–æ¨ç†å™¨
    logger.info(f"åˆå§‹åŒ–{model_config.model_name}æ¨ç†å™¨...")
    inferencer = LLMInference(model_config)

    # åŠ è½½æ•°æ®
    test_df = load_test_data(data_path)

    # å¦‚æœæŒ‡å®šäº†æ ·æœ¬æ•°ï¼Œåˆ™æŠ½æ ·
    if sample_size and sample_size < len(test_df):
        logger.info(f"ä»{len(test_df)}ä¸ªæ ·æœ¬ä¸­éšæœºæŠ½å–{sample_size}ä¸ª")
        test_df = test_df.sample(n=sample_size, random_state=42).reset_index(drop=True)

    # è¿›è¡Œæ¨ç†
    logger.info(f"å¼€å§‹æ¨ç†ï¼ˆå…±{len(test_df)}ä¸ªæ ·æœ¬ï¼‰...")
    predictions = []
    confidences = []
    raw_outputs = []

    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):
        result = inferencer.predict(row['context'], row['output'], use_zh=use_zh_prompt)
        predictions.append(result['prediction'])
        confidences.append(result['confidence'])
        raw_outputs.append(result['raw_output'])

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    y_true = test_df['label'].values
    y_pred = np.array(predictions)

    # è¿‡æ»¤å‡ºæœ‰æ•ˆé¢„æµ‹ï¼ˆæ’é™¤-1ï¼‰
    valid_mask = y_pred != -1
    y_true_valid = y_true[valid_mask]
    y_pred_valid = y_pred[valid_mask]

    logger.info(f"æœ‰æ•ˆé¢„æµ‹: {valid_mask.sum()}/{len(test_df)}")

    # è®¡ç®—æŒ‡æ ‡
    if len(y_pred_valid) > 0:
        metrics = calculate_detailed_metrics(y_true_valid, y_pred_valid)
    else:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return

    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    class_report = classification_report(
        y_true_valid, y_pred_valid,
        target_names=['No Hallucination', 'Hallucination'],
        output_dict=True
    )

    # ä¿å­˜ç»“æœ
    results = {
        'model_name': model_config.model_name,
        'deploy_type': model_config.deploy_type.value,
        'inference_time': datetime.now().isoformat(),
        'test_size': len(test_df),
        'valid_predictions': int(valid_mask.sum()),
        'use_zh_prompt': use_zh_prompt,
        'detailed_metrics': metrics,
        'classification_report': class_report
    }

    with open(f'{output_dir}/llm_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)

    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    results_df = test_df.copy()
    results_df['llm_prediction'] = y_pred
    results_df['llm_confidence'] = confidences
    results_df['llm_raw_output'] = raw_outputs
    results_df['correct_prediction'] = (y_pred == y_true)

    results_df.to_excel(f'{output_dir}/llm_detailed_predictions.xlsx', index=False)

    # æ‰“å°ç»“æœ
    print("\n" + "="*70)
    print(f"{model_config.model_name.upper()} å¹»è§‰æ£€æµ‹æ¨ç†ç»“æœ")
    print("="*70)

    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½æŒ‡æ ‡:")
    print(f"å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.4f}")
    print(f"å®å¹³å‡ç²¾ç¡®ç‡: {metrics['macro_precision']:.4f}")
    print(f"å®å¹³å‡å¬å›ç‡: {metrics['macro_recall']:.4f}")
    print(f"å®å¹³å‡F1åˆ†æ•°: {metrics['macro_f1']:.4f}")

    print(f"\nğŸ” æ— å¹»è§‰ç±»åˆ« (æ ‡ç­¾0):")
    no_hall = metrics['no_hallucination']
    print(f"ç²¾ç¡®ç‡: {no_hall['precision']:.4f}, å¬å›ç‡: {no_hall['recall']:.4f}, F1: {no_hall['f1_score']:.4f}")

    print(f"\nâš ï¸  æœ‰å¹»è§‰ç±»åˆ« (æ ‡ç­¾1):")
    hall = metrics['hallucination']
    print(f"ç²¾ç¡®ç‡: {hall['precision']:.4f}, å¬å›ç‡: {hall['recall']:.4f}, F1: {hall['f1_score']:.4f}")

    print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
    print(f"æ•æ„Ÿæ€§ (Sensitivity): {metrics['sensitivity']:.4f}")
    print(f"ç‰¹å¼‚æ€§ (Specificity): {metrics['specificity']:.4f}")

    cm = metrics['confusion_matrix']
    print(f"\nğŸ”¢ æ··æ·†çŸ©é˜µ:")
    print(f"çœŸé˜´æ€§ (TN): {cm['true_negatives']}, å‡é˜³æ€§ (FP): {cm['false_positives']}")
    print(f"å‡é˜´æ€§ (FN): {cm['false_negatives']}, çœŸé˜³æ€§ (TP): {cm['true_positives']}")

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print("="*70)

    return metrics, results


def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨LLMè¿›è¡Œå¹»è§‰æ£€æµ‹æ¨ç†')

    # æ¨¡å‹é…ç½®
    parser.add_argument('--model_name', required=True,
                       help='æ¨¡å‹åç§° (e.g., llama2, qwen, llama2-chat)')
    parser.add_argument('--deploy_type', default='huggingface',
                       choices=['ollama', 'huggingface', 'api'],
                       help='æ¨¡å‹éƒ¨ç½²ç±»å‹')
    parser.add_argument('--model_path', default=None,
                       help='HuggingFaceæ¨¡å‹è·¯å¾„æˆ–æœ¬åœ°æ¨¡å‹è·¯å¾„')
    parser.add_argument('--ollama_url', default='http://localhost:11434',
                       help='OllamaæœåŠ¡åœ°å€')

    # æ•°æ®é…ç½®
    parser.add_argument('--data_path', default='../summary_nli_hallucination_dataset.xlsx',
                       help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--sample_size', type=int, default=None,
                       help='æŠ½æ ·å¤§å°ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰')

    # æ¨ç†é…ç½®
    parser.add_argument('--output_dir', default='./llm_results',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--use_zh_prompt', action='store_true',
                       help='ä½¿ç”¨ä¸­æ–‡æç¤ºè¯')
    parser.add_argument('--temperature', type=float, default=0.0,
                       help='é‡‡æ ·æ¸©åº¦')
    parser.add_argument('--max_tokens', type=int, default=100,
                       help='æœ€å¤§ç”Ÿæˆtokenæ•°')

    # APIé…ç½®
    parser.add_argument('--api_key', default=None,
                       help='APIå¯†é’¥')
    parser.add_argument('--api_url', default=None,
                       help='APIåœ°å€')

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®
    config = LLMConfig(
        model_name=args.model_name,
        deploy_type=ModelDeployType(args.deploy_type),
        model_path=args.model_path,
        ollama_url=args.ollama_url,
        api_key=args.api_key,
        api_url=args.api_url,
        temperature=args.temperature,
        max_tokens=args.max_tokens
    )

    # è¿è¡Œæ¨ç†
    run_inference(
        model_config=config,
        data_path=args.data_path,
        output_dir=args.output_dir,
        use_zh_prompt=args.use_zh_prompt,
        sample_size=args.sample_size
    )


if __name__ == "__main__":
    main()
